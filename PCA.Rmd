---
title: "PCA"
output: 
   html_document:
      code_folding: hide
---

```{r  message=FALSE, warning=FALSE}

library(mgcv)
library(car)
library(parallel)
library(dplyr)
library(data.table)

datasetTrainPCA <-  readRDS('datasetTrainModeloClasificador.rds')
datasetTestPCA <-   readRDS('datasetTestModeloClasificador.rds')


names(datasetTrainPCA) <- make.names(names(datasetTrainPCA))
names(datasetTestPCA) <- make.names(names(datasetTestPCA))


var_eliminar_correlacionPCA = c("Street","Utilities","LandSlope","Condition2","BsmtFinSF2", 
"LowQualFinSF","MiscFeature","MiscVal","TotRmsAbvGrd","GarageYrBlt","GarageCars")  


columnas_continuas_PCA = c("LotArea","MasVnrArea","BsmtFinSF1","BsmtFinSF2","BsmtUnfSF" ,"TotalBsmtSF","LowQualFinSF","GrLivArea", "GarageArea","WoodDeckSF","OpenPorchSF","EnclosedPorch","PoolArea","MiscVal","SalePrice")

#var_modelo_with_SalePricePCA = c(var_modeloPCA,'SalePrice')


dataSetTrainPCA  <-   datasetTrainPCA    %>%    dplyr::select(-c('GrupoPrecio'))   %>% na.omit()
dataSetTestPCA  <-  datasetTestPCA  %>%   dplyr::select(-c('GrupoPrecio'))  %>%  na.omit()


```



Intentaremos hacer una descomposición PCA de los datos del dataset para ver si es viable en nuestro dataset simplificar el número de variables a utilizar en los modelos (sobre todos en modelos que no son explicativos, tal y como SVM, y en los que ayuda esta reducción de dimensionalidad en los recursos computacionales utilizados por dicho modelo). Como tenemos variables de todo tipo (discretas, contínuas, categóricas, ordinales) utilizarmos una librería denominada PCAmixdata que es capaz de generar esta descomposición en valores principales mezclando todos los tipos de variable. Para ello, previamente hay que descomponer el dataset en variables de tipo cualitativas y cuantitativas. Para ello la propia librería tiene una función que automatiza esta tarea.
El siguiente código realiza esta descomposición, tanto en el dataset completo como únicamente en los datos del dataset de Train:

```{r  message=FALSE, warning=FALSE}

library(PCAmixdata)
splitCompleto <- splitmix(dataSetTrainPCA)
X1SplitCompleto <- splitCompleto$X.quanti 
X2SplitCompleto <- splitCompleto$X.quali 
objetoPCAMIXCompleto <- PCAmix(X.quanti=X1SplitCompleto, X.quali=X2SplitCompleto,rename.level=TRUE, graph=FALSE,ndim=60)

```

Mostramos un grafico con el incremento de varianza explicada por cada uno de los autoectores obtenidos
en el anterior proceso:

```{r message=FALSE, warning=FALSE}

plot(objetoPCAMIXCompleto$eig[,3], xlab = 'Numero de Autovalor', ylab = 'Varianza Acumulada', main = 'Varianza cumulada por cada uno de los eigenvalues')
lines(objetoPCAMIXCompleto$eig[,3])

```


Como vemos, con el 60% de las variables obtenemos una varianza acumulado del 80%.  No obstante, si vemos el número de dimenosiones, vemos que para calcular este tipo de PCA se ha realizado un "one hot" encoding de las variables categóricas, y esto para alguno de los modelos (random forest, knn) no es conveniente.  No obstante, para el modelo basado en SVM se podría probar a ver si con esta reducción de la dimensionalidad conseguimos resultados equivalentes. EN los modelos GLM y GAM, preferimos no utilizar este tipo de codificación debido a que se pierde interpretabilildad en el modelo.

Como ejemplo del uso de estas técnicas en un modelo de ML, entrenaremos un RF rápido para comprobar si la reducción de la dimensionalidad conlleva un modelo con similares poderes de predicción:

```{r  message=FALSE, warning=FALSE}
numeroDimOptimo = 60

 

splitTest <- splitmix(datasetTestPCA%>% dplyr::select(-c(GrupoPrecio)))
X1Test <- splitTest$X.quanti 
X2Test <- splitTest$X.quali 

#objetoPCAMIXTrain$eig
coordenadasPCATrain=as.data.table(cbind(objetoPCAMIXCompleto$ind$coord, GrupoPrecio= datasetTrainPCA %>% dplyr::select(c("GrupoPrecio"))  ))

coordenadasPCATest=as.data.table(cbind(predict(objetoPCAMIXCompleto,X1Test,X2Test), GrupoPrecio=datasetTestPCA %>% dplyr::select(c("GrupoPrecio")) ))

# ahora la regresion por random forest con los componentes PCA

library(caret)

ctrl = trainControl(method="repeatedcv",
                    number=2,
                    repeats=1)

tGrid <-  expand.grid(mtry = c(7))

rf_model_pca <- train(GrupoPrecio ~.,
                data=coordenadasPCATrain,          
                method="rf",
                nodesize= 30,
                ntree =500,
                #do.trace= 10,
                trControl=ctrl,
                tuneGrid = tGrid,
                verbose = FALSE
                 )


# y por último , genero el mismo random forest con todos los componenes
rf_model_sin_pca <-train(GrupoPrecio ~.,
                data=datasetTrainPCA,          
                method="rf",
                nodesize= 30,
                ntree =500,
                #do.trace= 10,
                trControl=ctrl,
                tuneGrid = tGrid,
                verbose = FALSE,
                 )

rf_model_pca$results
rf_model_sin_pca$results

```

  Comprobamos que el accuracy en ambos modelos es similar con un 40% de reducción en el número de variables. En los comentarios de la práctica se ha comentado que el valor kappa debería no ser tan distinto en los dos modelos, que parece indicar que se han hecho los modelos sobre poblaciones disimilares. No obstante, después de revisar el código, se ha llegado a la conclusión de que ambos se realizan sobre la población de Train, lo único que uno de los modelos utiliza las componentes PCA y el otro no. No logro entender el por qué de estos valores tan disimilares.


