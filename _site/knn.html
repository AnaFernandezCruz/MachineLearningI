<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>K Nearest Neighbors - KNN</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">ML</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="Cluster.html">Clustering</a>
</li>
<li>
  <a href="PCA.html">PCA</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Aprendizaje supervisado
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="glm.html">GLM</a>
    </li>
    <li>
      <a href="knn.html">KNN</a>
    </li>
    <li>
      <a href="decision_trees.html">Decision Trees</a>
    </li>
    <li>
      <a href="random_forest.html">Random Forest</a>
    </li>
    <li>
      <a href="svm.html">SVM</a>
    </li>
  </ul>
</li>
<li>
  <a href="GAM.html">GAM</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">K Nearest Neighbors - KNN</h1>

</div>


<p>A continuación aplicaremos un modelo knn o k nearest neighbors a nuestro grupo de datos. El algoritmo clasifica cada dato nuevo en el grupo que corresponda, según tenga “k” vecinos más cerca de un grupo o de otro. Es decir, calcula la distancia del elemento nuevo a cada uno de los existentes, y ordena dichas distancias de menor a mayor para ir seleccionando el grupo al que pertenece. Este grupo será, por tanto, el de mayor frecuencia con menores distancias.</p>
<p>El knn es un algoritmo de aprendizaje supervisado, es decir, que a partir de un juego de datos inicial su objetivo será el de clasificar correctamente todas las instancias nuevas.</p>
<p>Cargamos las librerias y los datos que utilizaremos para el entrenamiento, con el conjunto de train, y la evaluación del modelo con el conjunto de test.</p>
<pre class="r"><code>library(class)
library(dplyr)
library(caret)
library (ROCR)
library(MASS)
library(hmeasure)
library(data.table)
source(&quot;funcs.R&quot;)
data(Pima.te)</code></pre>
<pre class="r"><code>dataTrain_origin &lt;- readRDS(&quot;datasetTrainModeloClasificador.rds&quot;)
dataTest_origin &lt;- readRDS(&quot;datasetTestModeloClasificador.rds&quot;)</code></pre>
<p>Procedemos a despejar de los dataset (train y test) la variable objetivo original SalePrice. De la cual ya hemos creamos la variable GrupoPrecio, del tipo categorica. Separando asi por una parte un grupo de casas baratas y otro grupo de casas caras.</p>
<p>Nuestro modelo clasificará casas entre estos dos grupos.</p>
<pre class="r"><code>dataTrain &lt;- dataTrain_origin %&gt;% dplyr::select(-SalePrice)
dataTest &lt;- dataTest_origin %&gt;% dplyr::select(-SalePrice)</code></pre>
<p>Para este modelo se han elegido un grupo de caracteristica del dataset original. Se han escogido debido a un analisis previo del conjunto de datos en el que mediante un modelo random forest se ha determinado el grupo de caracteristicas más importantes. A continuación preparamos los dataset para entrenar el modelo y posteriormente evaluar como se comporta con el conjunto de test. Debido a que nuestros datos tiene variables categoricas, tenemos que transformarlas a numericas para el modelo knn.</p>
<pre class="r"><code>remove &lt;- c(&#39;GrupoPrecio&#39;)
col_to_factor &lt;- colnames(dataTrain) [! colnames(dataTrain) %in% remove]

dataTrain &lt;- dataTrain %&gt;% as_factor_all(col_to_factor)
dataTest &lt;- dataTest %&gt;% as_factor_all(col_to_factor)

XTrain &lt;- dataTrain %&gt;% dplyr::select(-GrupoPrecio)
YTrain &lt;- dataTrain$GrupoPrecio

XTest &lt;- dataTest %&gt;% dplyr::select(-GrupoPrecio)
YTest &lt;- dataTest$GrupoPrecio</code></pre>
<div id="entrenamiento-optimización-y-evaluación-del-modelo" class="section level3">
<h3>ENTRENAMIENTO, OPTIMIZACIÓN Y EVALUACIÓN DEL MODELO</h3>
<p>Entrenamos el modelo a la vez que buscamos el k más optimo para el conjunto de datos de train normalizados.</p>
<pre class="r"><code>#Normalización
ctrNorm &lt;- preProcess(x = XTrain, method = c(&quot;center&quot;, &quot;scale&quot;))

dataTrainNorm &lt;- predict(ctrNorm, dataTrain)

#Entrenamiento y busqueda de k más optimo
set.seed(400)
ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;,repeats = 3)
knn &lt;- train(GrupoPrecio ~ ., data = dataTrainNorm, method = &quot;knn&quot;, trControl = ctrl, tuneLength = 20)
(knn)</code></pre>
<pre><code>## k-Nearest Neighbors 
## 
## 1837 samples
##   26 predictor
##    2 classes: &#39;Barato&#39;, &#39;Caro&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 1653, 1654, 1653, 1653, 1654, 1653, ... 
## Resampling results across tuning parameters:
## 
##   k   Accuracy   Kappa    
##    5  0.9261533  0.3377619
##    7  0.9261444  0.3015029
##    9  0.9274075  0.3157677
##   11  0.9292251  0.3208576
##   13  0.9304912  0.3257051
##   15  0.9312178  0.3286805
##   17  0.9310396  0.3117905
##   19  0.9292271  0.2855108
##   21  0.9310426  0.2933578
##   23  0.9306763  0.2801516
##   25  0.9310416  0.2734958
##   27  0.9301319  0.2571327
##   29  0.9281371  0.2256035
##   31  0.9281371  0.2207408
##   33  0.9263236  0.1765829
##   35  0.9250525  0.1504705
##   37  0.9252346  0.1442550
##   39  0.9261414  0.1593522
##   41  0.9245080  0.1270529
##   43  0.9243268  0.1129338
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was k = 15.</code></pre>
<pre class="r"><code>plot(knn)</code></pre>
<p><img src="knn_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>A continuación evaluamos nuestro modelo con el conjunto de test normalizados con la información de train.</p>
<pre class="r"><code>#Normalizamos con la información de train.
XTestNorm &lt;- predict(ctrNorm, XTest)
pred &lt;- predict(knn, newdata = XTestNorm )</code></pre>
<pre class="r"><code>cm &lt;- confusionMatrix(pred, YTest, mode = &quot;prec_recall&quot; )
(cm)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction Barato Caro
##     Barato    708   49
##     Caro       18   14
##                                           
##                Accuracy : 0.9151          
##                  95% CI : (0.8934, 0.9336)
##     No Information Rate : 0.9202          
##     P-Value [Acc &gt; NIR] : 0.7267359       
##                                           
##                   Kappa : 0.2546          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.0002473       
##                                           
##               Precision : 0.9353          
##                  Recall : 0.9752          
##                      F1 : 0.9548          
##              Prevalence : 0.9202          
##          Detection Rate : 0.8973          
##    Detection Prevalence : 0.9594          
##       Balanced Accuracy : 0.5987          
##                                           
##        &#39;Positive&#39; Class : Barato          
## </code></pre>
<pre class="r"><code>tab_test &lt;- table(pred, YTest, dnn = c(&quot;Actual&quot;, &quot;Predichos&quot;))
draw_confusion_matrix(tab_test, &quot;Actual&quot;, &quot;Predichos&quot;)</code></pre>
<p><img src="knn_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<div id="aplicamos-pca" class="section level5">
<h5>APLICAMOS PCA</h5>
<pre class="r"><code>dataTrain_origin_PCA &lt;- readRDS(&quot;datasetTrainModeloClasificadorPCA.rds&quot;)
dataTest_origin_PCA &lt;- readRDS(&quot;datasetTestModeloClasificadorPCA.rds&quot;)

PCATrain &lt;- as.data.table(cbind(dataTrain_origin_PCA$ind$coord, GrupoPrecio = dataTrain_origin %&gt;% dplyr::select(c(&quot;GrupoPrecio&quot;))))
PCATest &lt;- as.data.table(cbind(dataTest_origin_PCA$ind$coord, GrupoPrecio = dataTest_origin %&gt;% dplyr::select(c(&quot;GrupoPrecio&quot;))))

XPCATrain &lt;- PCATrain %&gt;% dplyr::select(-GrupoPrecio)
YPCATrain &lt;- PCATrain$GrupoPrecio
XPCATest &lt;- PCATest %&gt;% dplyr::select(-GrupoPrecio)
YPCATest &lt;- PCATest$GrupoPrecio

set.seed(400)
ctrl_pca &lt;- trainControl(method=&quot;repeatedcv&quot;,repeats = 3)
knn_pca &lt;- train(GrupoPrecio ~ ., data = PCATrain, method = &quot;knn&quot;, trControl = ctrl_pca, tuneLength = 20)
pred_pca &lt;- predict(knn_pca, newdata = XPCATest)</code></pre>
<p>Podemos observar que no obtenemos un mejor resultado que con el conjunto de datos original.</p>
<pre class="r"><code>cm_pca &lt;- confusionMatrix(pred_pca, YPCATest, mode = &quot;prec_recall&quot; )
(cm_pca)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction Barato Caro
##     Barato    712   47
##     Caro       14   16
##                                           
##                Accuracy : 0.9227          
##                  95% CI : (0.9018, 0.9403)
##     No Information Rate : 0.9202          
##     P-Value [Acc &gt; NIR] : 0.4289          
##                                           
##                   Kappa : 0.3085          
##                                           
##  Mcnemar&#39;s Test P-Value : 4.182e-05       
##                                           
##               Precision : 0.9381          
##                  Recall : 0.9807          
##                      F1 : 0.9589          
##              Prevalence : 0.9202          
##          Detection Rate : 0.9024          
##    Detection Prevalence : 0.9620          
##       Balanced Accuracy : 0.6173          
##                                           
##        &#39;Positive&#39; Class : Barato          
## </code></pre>
<pre class="r"><code>tab_test_pca &lt;- table(pred_pca, YPCATest, dnn = c(&quot;Actual&quot;, &quot;Predichos&quot;))
draw_confusion_matrix(tab_test_pca, &quot;Actual&quot;, &quot;Predichos&quot;)</code></pre>
<p><img src="knn_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code>###########################################################

ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;,repeats = 3, classProbs = TRUE, summaryFunction = twoClassSummary)
knnFit &lt;- train(GrupoPrecio ~ ., data = dataTrain, method = &quot;knn&quot;, trControl = ctrl, preProcess = c(&quot;center&quot;,&quot;scale&quot;), tuneLength = 20)</code></pre>
<pre><code>## Warning in train.default(x, y, weights = w, ...): The metric &quot;Accuracy&quot; was not
## in the result set. ROC will be used instead.</code></pre>
<pre class="r"><code>knnFit</code></pre>
<pre><code>## k-Nearest Neighbors 
## 
## 1837 samples
##   26 predictor
##    2 classes: &#39;Barato&#39;, &#39;Caro&#39; 
## 
## Pre-processing: centered (26), scaled (26) 
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 1653, 1653, 1653, 1653, 1654, 1653, ... 
## Resampling results across tuning parameters:
## 
##   k   ROC        Sens       Spec      
##    5  0.8373173  0.9776111  0.28095238
##    7  0.8776956  0.9793723  0.26666667
##    9  0.9005617  0.9821279  0.25714286
##   11  0.9057578  0.9840898  0.26190476
##   13  0.9141958  0.9856619  0.26428571
##   15  0.9178354  0.9870333  0.27142857
##   17  0.9175661  0.9872305  0.25000000
##   19  0.9185989  0.9876227  0.22857143
##   21  0.9203183  0.9876227  0.23333333
##   23  0.9209962  0.9886042  0.21904762
##   25  0.9213162  0.9909618  0.17619048
##   27  0.9228138  0.9917473  0.16666667
##   29  0.9272297  0.9919445  0.14523810
##   31  0.9291545  0.9919445  0.14285714
##   33  0.9307426  0.9931233  0.12380952
##   35  0.9322683  0.9933159  0.09761905
##   37  0.9312559  0.9935120  0.09523810
##   39  0.9319090  0.9935120  0.08809524
##   41  0.9351784  0.9942975  0.06904762
##   43  0.9357055  0.9950853  0.06190476
## 
## ROC was used to select the optimal model using the largest value.
## The final value used for the model was k = 43.</code></pre>
<pre class="r"><code>plot(knnFit, print.thres = 0.5, type=&quot;S&quot;)</code></pre>
<p><img src="knn_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre class="r"><code>knnPredict &lt;- predict(knnFit,newdata = dataTest )
#Get the confusion matrix to see accuracy value and other parameter values
confusionMatrix(knnPredict, YTest )</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction Barato Caro
##     Barato    718   56
##     Caro        8    7
##                                          
##                Accuracy : 0.9189         
##                  95% CI : (0.8976, 0.937)
##     No Information Rate : 0.9202         
##     P-Value [Acc &gt; NIR] : 0.585          
##                                          
##                   Kappa : 0.1535         
##                                          
##  Mcnemar&#39;s Test P-Value : 4.228e-09      
##                                          
##             Sensitivity : 0.9890         
##             Specificity : 0.1111         
##          Pos Pred Value : 0.9276         
##          Neg Pred Value : 0.4667         
##              Prevalence : 0.9202         
##          Detection Rate : 0.9100         
##    Detection Prevalence : 0.9810         
##       Balanced Accuracy : 0.5500         
##                                          
##        &#39;Positive&#39; Class : Barato         
## </code></pre>
</div>
<div id="curva-roc" class="section level5">
<h5>CURVA ROC</h5>
<pre class="r"><code>library(pROC)</code></pre>
<pre><code>## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation.</code></pre>
<pre><code>## 
## Attaching package: &#39;pROC&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     cov, smooth, var</code></pre>
<pre class="r"><code>knnPredict &lt;- predict(knnFit, newdata = dataTest , type=&quot;prob&quot;)

knnPredict</code></pre>
<pre><code>##        Barato       Caro
## 1   1.0000000 0.00000000
## 2   0.9767442 0.02325581
## 3   1.0000000 0.00000000
## 4   0.6511628 0.34883721
## 5   1.0000000 0.00000000
## 6   1.0000000 0.00000000
## 7   1.0000000 0.00000000
## 8   1.0000000 0.00000000
## 9   1.0000000 0.00000000
## 10  0.8837209 0.11627907
## 11  0.7674419 0.23255814
## 12  0.8372093 0.16279070
## 13  0.6511628 0.34883721
## 14  1.0000000 0.00000000
## 15  0.9767442 0.02325581
## 16  0.4186047 0.58139535
## 17  1.0000000 0.00000000
## 18  0.9534884 0.04651163
## 19  0.7209302 0.27906977
## 20  1.0000000 0.00000000
## 21  1.0000000 0.00000000
## 22  0.8837209 0.11627907
## 23  1.0000000 0.00000000
## 24  1.0000000 0.00000000
## 25  0.9767442 0.02325581
## 26  1.0000000 0.00000000
## 27  0.9534884 0.04651163
## 28  1.0000000 0.00000000
## 29  1.0000000 0.00000000
## 30  1.0000000 0.00000000
## 31  0.9767442 0.02325581
## 32  1.0000000 0.00000000
## 33  0.6511628 0.34883721
## 34  0.9069767 0.09302326
## 35  1.0000000 0.00000000
## 36  1.0000000 0.00000000
## 37  0.9767442 0.02325581
## 38  0.9534884 0.04651163
## 39  0.9767442 0.02325581
## 40  1.0000000 0.00000000
## 41  0.8837209 0.11627907
## 42  1.0000000 0.00000000
## 43  0.5581395 0.44186047
## 44  1.0000000 0.00000000
## 45  0.9302326 0.06976744
## 46  0.8837209 0.11627907
## 47  0.9302326 0.06976744
## 48  0.6279070 0.37209302
## 49  1.0000000 0.00000000
## 50  0.7674419 0.23255814
## 51  0.9767442 0.02325581
## 52  1.0000000 0.00000000
## 53  1.0000000 0.00000000
## 54  1.0000000 0.00000000
## 55  1.0000000 0.00000000
## 56  1.0000000 0.00000000
## 57  0.9534884 0.04651163
## 58  0.9069767 0.09302326
## 59  0.9767442 0.02325581
## 60  0.6744186 0.32558140
## 61  1.0000000 0.00000000
## 62  0.9767442 0.02325581
## 63  0.9534884 0.04651163
## 64  1.0000000 0.00000000
## 65  1.0000000 0.00000000
## 66  1.0000000 0.00000000
## 67  0.9069767 0.09302326
## 68  0.7906977 0.20930233
## 69  0.8372093 0.16279070
## 70  1.0000000 0.00000000
## 71  1.0000000 0.00000000
## 72  1.0000000 0.00000000
## 73  0.6976744 0.30232558
## 74  1.0000000 0.00000000
## 75  1.0000000 0.00000000
## 76  0.6976744 0.30232558
## 77  0.8604651 0.13953488
## 78  1.0000000 0.00000000
## 79  1.0000000 0.00000000
## 80  0.9534884 0.04651163
## 81  1.0000000 0.00000000
## 82  0.9069767 0.09302326
## 83  1.0000000 0.00000000
## 84  1.0000000 0.00000000
## 85  0.8139535 0.18604651
## 86  1.0000000 0.00000000
## 87  0.7906977 0.20930233
## 88  1.0000000 0.00000000
## 89  0.9534884 0.04651163
## 90  1.0000000 0.00000000
## 91  1.0000000 0.00000000
## 92  1.0000000 0.00000000
## 93  0.8604651 0.13953488
## 94  1.0000000 0.00000000
## 95  1.0000000 0.00000000
## 96  1.0000000 0.00000000
## 97  1.0000000 0.00000000
## 98  1.0000000 0.00000000
## 99  1.0000000 0.00000000
## 100 1.0000000 0.00000000
## 101 0.8604651 0.13953488
## 102 1.0000000 0.00000000
## 103 0.4418605 0.55813953
## 104 0.9767442 0.02325581
## 105 1.0000000 0.00000000
## 106 1.0000000 0.00000000
## 107 0.7906977 0.20930233
## 108 1.0000000 0.00000000
## 109 1.0000000 0.00000000
## 110 1.0000000 0.00000000
## 111 1.0000000 0.00000000
## 112 1.0000000 0.00000000
## 113 1.0000000 0.00000000
## 114 1.0000000 0.00000000
## 115 0.9767442 0.02325581
## 116 1.0000000 0.00000000
## 117 1.0000000 0.00000000
## 118 1.0000000 0.00000000
## 119 0.9767442 0.02325581
## 120 1.0000000 0.00000000
## 121 1.0000000 0.00000000
## 122 1.0000000 0.00000000
## 123 0.8139535 0.18604651
## 124 0.9534884 0.04651163
## 125 1.0000000 0.00000000
## 126 0.9534884 0.04651163
## 127 1.0000000 0.00000000
## 128 0.6046512 0.39534884
## 129 0.6511628 0.34883721
## 130 1.0000000 0.00000000
## 131 1.0000000 0.00000000
## 132 1.0000000 0.00000000
## 133 1.0000000 0.00000000
## 134 0.9767442 0.02325581
## 135 1.0000000 0.00000000
## 136 0.8372093 0.16279070
## 137 1.0000000 0.00000000
## 138 0.5348837 0.46511628
## 139 0.5348837 0.46511628
## 140 0.8604651 0.13953488
## 141 1.0000000 0.00000000
## 142 1.0000000 0.00000000
## 143 0.8372093 0.16279070
## 144 1.0000000 0.00000000
## 145 0.9767442 0.02325581
## 146 1.0000000 0.00000000
## 147 1.0000000 0.00000000
## 148 0.8181818 0.18181818
## 149 1.0000000 0.00000000
## 150 1.0000000 0.00000000
## 151 1.0000000 0.00000000
## 152 1.0000000 0.00000000
## 153 1.0000000 0.00000000
## 154 1.0000000 0.00000000
## 155 1.0000000 0.00000000
## 156 1.0000000 0.00000000
## 157 0.8837209 0.11627907
## 158 1.0000000 0.00000000
## 159 1.0000000 0.00000000
## 160 0.7441860 0.25581395
## 161 0.9069767 0.09302326
## 162 0.9534884 0.04651163
## 163 0.8604651 0.13953488
## 164 0.9069767 0.09302326
## 165 1.0000000 0.00000000
## 166 1.0000000 0.00000000
## 167 0.7441860 0.25581395
## 168 0.7441860 0.25581395
## 169 1.0000000 0.00000000
## 170 1.0000000 0.00000000
## 171 0.9534884 0.04651163
## 172 1.0000000 0.00000000
## 173 1.0000000 0.00000000
## 174 0.5581395 0.44186047
## 175 1.0000000 0.00000000
## 176 0.9767442 0.02325581
## 177 1.0000000 0.00000000
## 178 1.0000000 0.00000000
## 179 1.0000000 0.00000000
## 180 1.0000000 0.00000000
## 181 0.5813953 0.41860465
## 182 1.0000000 0.00000000
## 183 1.0000000 0.00000000
## 184 1.0000000 0.00000000
## 185 1.0000000 0.00000000
## 186 1.0000000 0.00000000
## 187 0.5348837 0.46511628
## 188 0.9069767 0.09302326
## 189 0.9534884 0.04651163
## 190 1.0000000 0.00000000
## 191 1.0000000 0.00000000
## 192 0.9534884 0.04651163
## 193 1.0000000 0.00000000
## 194 0.9767442 0.02325581
## 195 0.9069767 0.09302326
## 196 0.9534884 0.04651163
## 197 0.3953488 0.60465116
## 198 1.0000000 0.00000000
## 199 1.0000000 0.00000000
## 200 0.9069767 0.09302326
## 201 1.0000000 0.00000000
## 202 0.9302326 0.06976744
## 203 1.0000000 0.00000000
## 204 0.9302326 0.06976744
## 205 1.0000000 0.00000000
## 206 1.0000000 0.00000000
## 207 1.0000000 0.00000000
## 208 0.8372093 0.16279070
## 209 1.0000000 0.00000000
## 210 0.6976744 0.30232558
## 211 1.0000000 0.00000000
## 212 1.0000000 0.00000000
## 213 0.9534884 0.04651163
## 214 1.0000000 0.00000000
## 215 1.0000000 0.00000000
## 216 1.0000000 0.00000000
## 217 1.0000000 0.00000000
## 218 0.9534884 0.04651163
## 219 1.0000000 0.00000000
## 220 1.0000000 0.00000000
## 221 0.8372093 0.16279070
## 222 1.0000000 0.00000000
## 223 0.9767442 0.02325581
## 224 1.0000000 0.00000000
## 225 0.6511628 0.34883721
## 226 0.9767442 0.02325581
## 227 1.0000000 0.00000000
## 228 1.0000000 0.00000000
## 229 1.0000000 0.00000000
## 230 0.9302326 0.06976744
## 231 1.0000000 0.00000000
## 232 1.0000000 0.00000000
## 233 0.9767442 0.02325581
## 234 1.0000000 0.00000000
## 235 1.0000000 0.00000000
## 236 1.0000000 0.00000000
## 237 0.9302326 0.06976744
## 238 0.7209302 0.27906977
## 239 0.9302326 0.06976744
## 240 1.0000000 0.00000000
## 241 0.9767442 0.02325581
## 242 1.0000000 0.00000000
## 243 1.0000000 0.00000000
## 244 1.0000000 0.00000000
## 245 1.0000000 0.00000000
## 246 0.9534884 0.04651163
## 247 1.0000000 0.00000000
## 248 0.9534884 0.04651163
## 249 1.0000000 0.00000000
## 250 0.4883721 0.51162791
## 251 0.9767442 0.02325581
## 252 0.4418605 0.55813953
## 253 1.0000000 0.00000000
## 254 1.0000000 0.00000000
## 255 1.0000000 0.00000000
## 256 0.9767442 0.02325581
## 257 0.9767442 0.02325581
## 258 1.0000000 0.00000000
## 259 1.0000000 0.00000000
## 260 0.6744186 0.32558140
## 261 0.4883721 0.51162791
## 262 1.0000000 0.00000000
## 263 0.5813953 0.41860465
## 264 1.0000000 0.00000000
## 265 1.0000000 0.00000000
## 266 1.0000000 0.00000000
## 267 0.6279070 0.37209302
## 268 1.0000000 0.00000000
## 269 1.0000000 0.00000000
## 270 1.0000000 0.00000000
## 271 1.0000000 0.00000000
## 272 0.9069767 0.09302326
## 273 1.0000000 0.00000000
## 274 1.0000000 0.00000000
## 275 1.0000000 0.00000000
## 276 1.0000000 0.00000000
## 277 1.0000000 0.00000000
## 278 1.0000000 0.00000000
## 279 0.9534884 0.04651163
## 280 1.0000000 0.00000000
## 281 1.0000000 0.00000000
## 282 0.8604651 0.13953488
## 283 1.0000000 0.00000000
## 284 1.0000000 0.00000000
## 285 1.0000000 0.00000000
## 286 0.9767442 0.02325581
## 287 1.0000000 0.00000000
## 288 1.0000000 0.00000000
## 289 1.0000000 0.00000000
## 290 0.9302326 0.06976744
## 291 1.0000000 0.00000000
## 292 1.0000000 0.00000000
## 293 1.0000000 0.00000000
## 294 0.9767442 0.02325581
## 295 0.9069767 0.09302326
## 296 1.0000000 0.00000000
## 297 1.0000000 0.00000000
## 298 1.0000000 0.00000000
## 299 1.0000000 0.00000000
## 300 0.9767442 0.02325581
## 301 0.9767442 0.02325581
## 302 0.9302326 0.06976744
## 303 0.4651163 0.53488372
## 304 0.8604651 0.13953488
## 305 0.9534884 0.04651163
## 306 1.0000000 0.00000000
## 307 0.7674419 0.23255814
## 308 1.0000000 0.00000000
## 309 1.0000000 0.00000000
## 310 1.0000000 0.00000000
## 311 1.0000000 0.00000000
## 312 0.9302326 0.06976744
## 313 0.7441860 0.25581395
## 314 1.0000000 0.00000000
## 315 0.9534884 0.04651163
## 316 1.0000000 0.00000000
## 317 0.9534884 0.04651163
## 318 1.0000000 0.00000000
## 319 0.7209302 0.27906977
## 320 1.0000000 0.00000000
## 321 1.0000000 0.00000000
## 322 1.0000000 0.00000000
## 323 1.0000000 0.00000000
## 324 1.0000000 0.00000000
## 325 0.9069767 0.09302326
## 326 0.9767442 0.02325581
## 327 0.9767442 0.02325581
## 328 1.0000000 0.00000000
## 329 1.0000000 0.00000000
## 330 0.7209302 0.27906977
## 331 1.0000000 0.00000000
## 332 1.0000000 0.00000000
## 333 0.9534884 0.04651163
## 334 1.0000000 0.00000000
## 335 0.9302326 0.06976744
## 336 0.5348837 0.46511628
## 337 1.0000000 0.00000000
## 338 1.0000000 0.00000000
## 339 1.0000000 0.00000000
## 340 1.0000000 0.00000000
## 341 1.0000000 0.00000000
## 342 0.5581395 0.44186047
## 343 0.9302326 0.06976744
## 344 0.7441860 0.25581395
## 345 1.0000000 0.00000000
## 346 0.9069767 0.09302326
## 347 0.9534884 0.04651163
## 348 0.9302326 0.06976744
## 349 1.0000000 0.00000000
## 350 1.0000000 0.00000000
## 351 0.8139535 0.18604651
## 352 1.0000000 0.00000000
## 353 1.0000000 0.00000000
## 354 0.9767442 0.02325581
## 355 0.8837209 0.11627907
## 356 1.0000000 0.00000000
## 357 0.9767442 0.02325581
## 358 1.0000000 0.00000000
## 359 1.0000000 0.00000000
## 360 1.0000000 0.00000000
## 361 0.6279070 0.37209302
## 362 1.0000000 0.00000000
## 363 1.0000000 0.00000000
## 364 0.6511628 0.34883721
## 365 0.9534884 0.04651163
## 366 0.9767442 0.02325581
## 367 1.0000000 0.00000000
## 368 1.0000000 0.00000000
## 369 0.5348837 0.46511628
## 370 0.9767442 0.02325581
## 371 0.9767442 0.02325581
## 372 0.7906977 0.20930233
## 373 1.0000000 0.00000000
## 374 1.0000000 0.00000000
## 375 0.7674419 0.23255814
## 376 0.9069767 0.09302326
## 377 1.0000000 0.00000000
## 378 1.0000000 0.00000000
## 379 0.9767442 0.02325581
## 380 0.6976744 0.30232558
## 381 0.6279070 0.37209302
## 382 1.0000000 0.00000000
## 383 0.8372093 0.16279070
## 384 1.0000000 0.00000000
## 385 0.6511628 0.34883721
## 386 1.0000000 0.00000000
## 387 1.0000000 0.00000000
## 388 1.0000000 0.00000000
## 389 0.8837209 0.11627907
## 390 1.0000000 0.00000000
## 391 1.0000000 0.00000000
## 392 1.0000000 0.00000000
## 393 1.0000000 0.00000000
## 394 1.0000000 0.00000000
## 395 1.0000000 0.00000000
## 396 1.0000000 0.00000000
## 397 1.0000000 0.00000000
## 398 1.0000000 0.00000000
## 399 0.9767442 0.02325581
## 400 1.0000000 0.00000000
## 401 1.0000000 0.00000000
## 402 1.0000000 0.00000000
## 403 0.8604651 0.13953488
## 404 1.0000000 0.00000000
## 405 1.0000000 0.00000000
## 406 1.0000000 0.00000000
## 407 0.9302326 0.06976744
## 408 0.8604651 0.13953488
## 409 1.0000000 0.00000000
## 410 1.0000000 0.00000000
## 411 0.9534884 0.04651163
## 412 1.0000000 0.00000000
## 413 1.0000000 0.00000000
## 414 1.0000000 0.00000000
## 415 0.9534884 0.04651163
## 416 1.0000000 0.00000000
## 417 0.9069767 0.09302326
## 418 1.0000000 0.00000000
## 419 1.0000000 0.00000000
## 420 1.0000000 0.00000000
## 421 0.9302326 0.06976744
## 422 1.0000000 0.00000000
## 423 0.8604651 0.13953488
## 424 0.9302326 0.06976744
## 425 0.9534884 0.04651163
## 426 1.0000000 0.00000000
## 427 0.4883721 0.51162791
## 428 0.4883721 0.51162791
## 429 0.6279070 0.37209302
## 430 0.4651163 0.53488372
## 431 0.6046512 0.39534884
## 432 0.9302326 0.06976744
## 433 0.9767442 0.02325581
## 434 1.0000000 0.00000000
## 435 1.0000000 0.00000000
## 436 1.0000000 0.00000000
## 437 0.5813953 0.41860465
## 438 0.8372093 0.16279070
## 439 0.7674419 0.23255814
## 440 0.8372093 0.16279070
## 441 0.7906977 0.20930233
## 442 1.0000000 0.00000000
## 443 0.8837209 0.11627907
## 444 1.0000000 0.00000000
## 445 0.9302326 0.06976744
## 446 1.0000000 0.00000000
## 447 1.0000000 0.00000000
## 448 1.0000000 0.00000000
## 449 0.8837209 0.11627907
## 450 1.0000000 0.00000000
## 451 1.0000000 0.00000000
## 452 1.0000000 0.00000000
## 453 1.0000000 0.00000000
## 454 1.0000000 0.00000000
## 455 1.0000000 0.00000000
## 456 1.0000000 0.00000000
## 457 1.0000000 0.00000000
## 458 1.0000000 0.00000000
## 459 1.0000000 0.00000000
## 460 1.0000000 0.00000000
## 461 1.0000000 0.00000000
## 462 1.0000000 0.00000000
## 463 1.0000000 0.00000000
## 464 1.0000000 0.00000000
## 465 1.0000000 0.00000000
## 466 1.0000000 0.00000000
## 467 1.0000000 0.00000000
## 468 1.0000000 0.00000000
## 469 1.0000000 0.00000000
## 470 1.0000000 0.00000000
## 471 1.0000000 0.00000000
## 472 1.0000000 0.00000000
## 473 1.0000000 0.00000000
## 474 1.0000000 0.00000000
## 475 1.0000000 0.00000000
## 476 1.0000000 0.00000000
## 477 1.0000000 0.00000000
## 478 1.0000000 0.00000000
## 479 1.0000000 0.00000000
## 480 0.9767442 0.02325581
## 481 1.0000000 0.00000000
## 482 0.9767442 0.02325581
## 483 0.6511628 0.34883721
## 484 0.6279070 0.37209302
## 485 0.9767442 0.02325581
## 486 0.9302326 0.06976744
## 487 1.0000000 0.00000000
## 488 1.0000000 0.00000000
## 489 1.0000000 0.00000000
## 490 0.7674419 0.23255814
## 491 0.9534884 0.04651163
## 492 1.0000000 0.00000000
## 493 1.0000000 0.00000000
## 494 1.0000000 0.00000000
## 495 1.0000000 0.00000000
## 496 1.0000000 0.00000000
## 497 1.0000000 0.00000000
## 498 0.9767442 0.02325581
## 499 0.9069767 0.09302326
## 500 0.4651163 0.53488372
## 501 0.7441860 0.25581395
## 502 1.0000000 0.00000000
## 503 1.0000000 0.00000000
## 504 1.0000000 0.00000000
## 505 0.7906977 0.20930233
## 506 0.6046512 0.39534884
## 507 0.9772727 0.02272727
## 508 0.9069767 0.09302326
## 509 0.7906977 0.20930233
## 510 0.8139535 0.18604651
## 511 1.0000000 0.00000000
## 512 1.0000000 0.00000000
## 513 0.6046512 0.39534884
## 514 1.0000000 0.00000000
## 515 1.0000000 0.00000000
## 516 1.0000000 0.00000000
## 517 1.0000000 0.00000000
## 518 1.0000000 0.00000000
## 519 0.5581395 0.44186047
## 520 0.5581395 0.44186047
## 521 0.8837209 0.11627907
## 522 0.6279070 0.37209302
## 523 0.6976744 0.30232558
## 524 0.9534884 0.04651163
## 525 1.0000000 0.00000000
## 526 1.0000000 0.00000000
## 527 0.9302326 0.06976744
## 528 0.9069767 0.09302326
## 529 0.6511628 0.34883721
## 530 0.9534884 0.04651163
## 531 1.0000000 0.00000000
## 532 0.5116279 0.48837209
## 533 0.9534884 0.04651163
## 534 1.0000000 0.00000000
## 535 1.0000000 0.00000000
## 536 1.0000000 0.00000000
## 537 1.0000000 0.00000000
## 538 1.0000000 0.00000000
## 539 1.0000000 0.00000000
## 540 1.0000000 0.00000000
## 541 0.9069767 0.09302326
## 542 1.0000000 0.00000000
## 543 1.0000000 0.00000000
## 544 1.0000000 0.00000000
## 545 1.0000000 0.00000000
## 546 1.0000000 0.00000000
## 547 1.0000000 0.00000000
## 548 1.0000000 0.00000000
## 549 1.0000000 0.00000000
## 550 1.0000000 0.00000000
## 551 1.0000000 0.00000000
## 552 1.0000000 0.00000000
## 553 1.0000000 0.00000000
## 554 0.9767442 0.02325581
## 555 1.0000000 0.00000000
## 556 1.0000000 0.00000000
## 557 1.0000000 0.00000000
## 558 1.0000000 0.00000000
## 559 1.0000000 0.00000000
## 560 1.0000000 0.00000000
## 561 1.0000000 0.00000000
## 562 1.0000000 0.00000000
## 563 0.9767442 0.02325581
## 564 0.6046512 0.39534884
## 565 0.5348837 0.46511628
## 566 1.0000000 0.00000000
## 567 0.9534884 0.04651163
## 568 1.0000000 0.00000000
## 569 1.0000000 0.00000000
## 570 0.9767442 0.02325581
## 571 0.8604651 0.13953488
## 572 0.9534884 0.04651163
## 573 0.9767442 0.02325581
## 574 0.9534884 0.04651163
## 575 1.0000000 0.00000000
## 576 1.0000000 0.00000000
## 577 1.0000000 0.00000000
## 578 1.0000000 0.00000000
## 579 1.0000000 0.00000000
## 580 0.8837209 0.11627907
## 581 1.0000000 0.00000000
## 582 1.0000000 0.00000000
## 583 0.8837209 0.11627907
## 584 0.6744186 0.32558140
## 585 0.8372093 0.16279070
## 586 0.7209302 0.27906977
## 587 0.6279070 0.37209302
## 588 1.0000000 0.00000000
## 589 1.0000000 0.00000000
## 590 1.0000000 0.00000000
## 591 1.0000000 0.00000000
## 592 1.0000000 0.00000000
## 593 0.9767442 0.02325581
## 594 1.0000000 0.00000000
## 595 1.0000000 0.00000000
## 596 0.9534884 0.04651163
## 597 1.0000000 0.00000000
## 598 0.9767442 0.02325581
## 599 0.9534884 0.04651163
## 600 0.9767442 0.02325581
## 601 1.0000000 0.00000000
## 602 1.0000000 0.00000000
## 603 0.4651163 0.53488372
## 604 0.6976744 0.30232558
## 605 0.5581395 0.44186047
## 606 0.7500000 0.25000000
## 607 0.5581395 0.44186047
## 608 0.7209302 0.27906977
## 609 0.7441860 0.25581395
## 610 0.9302326 0.06976744
## 611 1.0000000 0.00000000
## 612 1.0000000 0.00000000
## 613 1.0000000 0.00000000
## 614 1.0000000 0.00000000
## 615 1.0000000 0.00000000
## 616 0.9302326 0.06976744
## 617 1.0000000 0.00000000
## 618 0.6976744 0.30232558
## 619 1.0000000 0.00000000
## 620 0.9767442 0.02325581
## 621 0.9069767 0.09302326
## 622 0.9069767 0.09302326
## 623 0.8604651 0.13953488
## 624 1.0000000 0.00000000
## 625 1.0000000 0.00000000
## 626 1.0000000 0.00000000
## 627 1.0000000 0.00000000
## 628 1.0000000 0.00000000
## 629 0.9767442 0.02325581
## 630 0.7209302 0.27906977
## 631 0.5813953 0.41860465
## 632 1.0000000 0.00000000
## 633 1.0000000 0.00000000
## 634 1.0000000 0.00000000
## 635 1.0000000 0.00000000
## 636 1.0000000 0.00000000
## 637 1.0000000 0.00000000
## 638 1.0000000 0.00000000
## 639 1.0000000 0.00000000
## 640 1.0000000 0.00000000
## 641 1.0000000 0.00000000
## 642 1.0000000 0.00000000
## 643 1.0000000 0.00000000
## 644 1.0000000 0.00000000
## 645 1.0000000 0.00000000
## 646 1.0000000 0.00000000
## 647 1.0000000 0.00000000
## 648 1.0000000 0.00000000
## 649 1.0000000 0.00000000
## 650 1.0000000 0.00000000
## 651 1.0000000 0.00000000
## 652 1.0000000 0.00000000
## 653 1.0000000 0.00000000
## 654 1.0000000 0.00000000
## 655 1.0000000 0.00000000
## 656 1.0000000 0.00000000
## 657 1.0000000 0.00000000
## 658 1.0000000 0.00000000
## 659 1.0000000 0.00000000
## 660 1.0000000 0.00000000
## 661 1.0000000 0.00000000
## 662 0.9767442 0.02325581
## 663 1.0000000 0.00000000
## 664 0.9534884 0.04651163
## 665 1.0000000 0.00000000
## 666 1.0000000 0.00000000
## 667 1.0000000 0.00000000
## 668 1.0000000 0.00000000
## 669 1.0000000 0.00000000
## 670 0.7906977 0.20930233
## 671 1.0000000 0.00000000
## 672 1.0000000 0.00000000
## 673 1.0000000 0.00000000
## 674 0.9534884 0.04651163
## 675 1.0000000 0.00000000
## 676 0.9534884 0.04651163
## 677 1.0000000 0.00000000
## 678 1.0000000 0.00000000
## 679 1.0000000 0.00000000
## 680 0.9302326 0.06976744
## 681 1.0000000 0.00000000
## 682 1.0000000 0.00000000
## 683 1.0000000 0.00000000
## 684 1.0000000 0.00000000
## 685 1.0000000 0.00000000
## 686 1.0000000 0.00000000
## 687 1.0000000 0.00000000
## 688 1.0000000 0.00000000
## 689 1.0000000 0.00000000
## 690 1.0000000 0.00000000
## 691 1.0000000 0.00000000
## 692 1.0000000 0.00000000
## 693 1.0000000 0.00000000
## 694 0.9069767 0.09302326
## 695 0.9767442 0.02325581
## 696 1.0000000 0.00000000
## 697 0.9772727 0.02272727
## 698 0.8372093 0.16279070
## 699 0.8604651 0.13953488
## 700 0.8604651 0.13953488
## 701 1.0000000 0.00000000
## 702 0.7441860 0.25581395
## 703 1.0000000 0.00000000
## 704 0.4883721 0.51162791
## 705 0.4418605 0.55813953
## 706 0.5581395 0.44186047
## 707 1.0000000 0.00000000
## 708 0.9767442 0.02325581
## 709 1.0000000 0.00000000
## 710 1.0000000 0.00000000
## 711 0.5813953 0.41860465
## 712 0.7906977 0.20930233
## 713 0.6976744 0.30232558
## 714 0.8372093 0.16279070
## 715 0.6976744 0.30232558
## 716 0.8604651 0.13953488
## 717 0.8139535 0.18604651
## 718 0.9534884 0.04651163
## 719 0.9767442 0.02325581
## 720 0.9534884 0.04651163
## 721 1.0000000 0.00000000
## 722 0.4883721 0.51162791
## 723 1.0000000 0.00000000
## 724 0.8372093 0.16279070
## 725 1.0000000 0.00000000
## 726 1.0000000 0.00000000
## 727 1.0000000 0.00000000
## 728 1.0000000 0.00000000
## 729 1.0000000 0.00000000
## 730 1.0000000 0.00000000
## 731 1.0000000 0.00000000
## 732 1.0000000 0.00000000
## 733 1.0000000 0.00000000
## 734 1.0000000 0.00000000
## 735 1.0000000 0.00000000
## 736 1.0000000 0.00000000
## 737 0.9534884 0.04651163
## 738 1.0000000 0.00000000
## 739 1.0000000 0.00000000
## 740 1.0000000 0.00000000
## 741 0.9767442 0.02325581
## 742 1.0000000 0.00000000
## 743 0.8409091 0.15909091
## 744 1.0000000 0.00000000
## 745 1.0000000 0.00000000
## 746 1.0000000 0.00000000
## 747 1.0000000 0.00000000
## 748 1.0000000 0.00000000
## 749 1.0000000 0.00000000
## 750 1.0000000 0.00000000
## 751 1.0000000 0.00000000
## 752 1.0000000 0.00000000
## 753 1.0000000 0.00000000
## 754 1.0000000 0.00000000
## 755 1.0000000 0.00000000
## 756 1.0000000 0.00000000
## 757 1.0000000 0.00000000
## 758 1.0000000 0.00000000
## 759 1.0000000 0.00000000
## 760 1.0000000 0.00000000
## 761 1.0000000 0.00000000
## 762 1.0000000 0.00000000
## 763 0.9534884 0.04651163
## 764 1.0000000 0.00000000
## 765 0.8139535 0.18604651
## 766 0.9302326 0.06976744
## 767 1.0000000 0.00000000
## 768 1.0000000 0.00000000
## 769 0.9767442 0.02325581
## 770 0.9302326 0.06976744
## 771 1.0000000 0.00000000
## 772 1.0000000 0.00000000
## 773 1.0000000 0.00000000
## 774 0.9534884 0.04651163
## 775 0.9767442 0.02325581
## 776 0.9069767 0.09302326
## 777 1.0000000 0.00000000
## 778 1.0000000 0.00000000
## 779 0.9767442 0.02325581
## 780 1.0000000 0.00000000
## 781 1.0000000 0.00000000
## 782 1.0000000 0.00000000
## 783 1.0000000 0.00000000
## 784 1.0000000 0.00000000
## 785 1.0000000 0.00000000
## 786 1.0000000 0.00000000
## 787 1.0000000 0.00000000
## 788 1.0000000 0.00000000
## 789 1.0000000 0.00000000</code></pre>
<pre class="r"><code>a &lt;- rev(YTest)
(a)</code></pre>
<pre><code>##   [1] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
##  [11] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
##  [21] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
##  [31] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
##  [41] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
##  [51] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
##  [61] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
##  [71] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
##  [81] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
##  [91] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [101] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [111] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [121] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [131] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [141] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [151] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [161] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [171] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [181] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [191] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [201] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [211] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [221] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [231] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [241] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [251] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [261] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [271] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [281] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [291] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [301] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [311] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [321] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [331] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [341] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [351] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [361] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [371] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [381] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [391] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [401] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [411] Barato Barato Barato Barato Caro   Barato Barato Barato Barato Barato
## [421] Caro   Barato Barato Caro   Barato Caro   Barato Barato Caro   Barato
## [431] Barato Barato Barato Barato Barato Caro   Barato Barato Barato Barato
## [441] Barato Barato Caro   Barato Barato Barato Barato Caro   Barato Barato
## [451] Barato Barato Barato Caro   Caro   Barato Barato Barato Barato Caro  
## [461] Barato Barato Barato Barato Caro   Barato Barato Barato Barato Barato
## [471] Caro   Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [481] Barato Barato Caro   Barato Barato Barato Caro   Barato Barato Barato
## [491] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [501] Barato Barato Barato Barato Barato Barato Caro   Barato Barato Barato
## [511] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [521] Barato Barato Caro   Barato Barato Barato Caro   Barato Caro   Caro  
## [531] Barato Barato Barato Barato Barato Barato Barato Caro   Barato Caro  
## [541] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [551] Barato Caro   Barato Barato Barato Barato Barato Barato Barato Barato
## [561] Barato Barato Barato Barato Caro   Barato Caro   Barato Barato Barato
## [571] Barato Barato Barato Barato Barato Barato Barato Barato Barato Caro  
## [581] Barato Caro   Barato Barato Barato Caro   Barato Barato Barato Barato
## [591] Barato Barato Caro   Barato Caro   Barato Barato Caro   Barato Barato
## [601] Barato Barato Caro   Barato Barato Barato Barato Barato Caro   Barato
## [611] Barato Barato Barato Barato Barato Caro   Barato Barato Caro   Barato
## [621] Barato Caro   Barato Barato Barato Caro   Caro   Barato Barato Caro  
## [631] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [641] Barato Caro   Barato Barato Barato Barato Caro   Barato Barato Barato
## [651] Caro   Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [661] Caro   Caro   Barato Barato Barato Barato Caro   Barato Barato Barato
## [671] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [681] Barato Barato Barato Barato Barato Barato Caro   Barato Barato Barato
## [691] Barato Barato Barato Barato Barato Barato Caro   Barato Barato Barato
## [701] Barato Barato Caro   Barato Caro   Barato Barato Barato Barato Barato
## [711] Barato Barato Barato Caro   Barato Barato Caro   Barato Barato Barato
## [721] Barato Barato Barato Barato Barato Barato Barato Barato Barato Caro  
## [731] Barato Caro   Barato Barato Barato Barato Barato Barato Barato Caro  
## [741] Barato Caro   Barato Barato Barato Barato Caro   Barato Caro   Barato
## [751] Barato Barato Barato Barato Barato Barato Caro   Barato Barato Barato
## [761] Barato Barato Barato Barato Barato Barato Barato Barato Barato Barato
## [771] Barato Barato Barato Caro   Barato Barato Caro   Barato Caro   Caro  
## [781] Barato Barato Barato Barato Barato Caro   Barato Barato Barato
## Levels: Barato Caro</code></pre>
<pre class="r"><code>knnROC &lt;- roc(YTest, knnPredict[,&quot;Caro&quot;], levels = c(&quot;Barato&quot;,&quot;Caro&quot;))</code></pre>
<pre><code>## Setting direction: controls &lt; cases</code></pre>
<pre class="r"><code>#knnROC

ROC &lt;- roc(YTest, knnPredict[,&quot;Caro&quot;], levels = c(&quot;Barato&quot;,&quot;Caro&quot;), ret = c(&quot;roc&quot;, &quot;coords&quot;, &quot;all_coords&quot;))</code></pre>
<pre><code>## Setting direction: controls &lt; cases</code></pre>
<pre class="r"><code>(ROC)</code></pre>
<pre><code>## 
## Call:
## roc.default(response = YTest, predictor = knnPredict[, &quot;Caro&quot;],     levels = c(&quot;Barato&quot;, &quot;Caro&quot;), ret = c(&quot;roc&quot;, &quot;coords&quot;, &quot;all_coords&quot;))
## 
## Data: knnPredict[, &quot;Caro&quot;] in 726 controls (YTest Barato) &lt; 63 cases (YTest Caro).
## Area under the curve: 0.9128</code></pre>
<pre class="r"><code>plot(ROC, type=&quot;S&quot;, print.thres = 0.5)
plot(knnROC, type=&quot;S&quot;, print.thres = 0.5)</code></pre>
<p><img src="knn_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>roc1 &lt;- roc(YTest, knnPredict[,&quot;Caro&quot;], percent=TRUE,
            # arguments for auc
            partial.auc = c(100, 90), partial.auc.correct=TRUE,
            partial.auc.focus = &quot;sens&quot;,
            # arguments for ci
            ci = TRUE, boot.n = 100, ci.alpha = 0.9, stratified = FALSE,
            # arguments for plot
            plot = TRUE, auc.polygon = TRUE, max.auc.polygon = TRUE, grid = TRUE,
            print.auc = TRUE, show.thres = TRUE)</code></pre>
<pre><code>## Setting levels: control = Barato, case = Caro</code></pre>
<pre><code>## Setting direction: controls &lt; cases</code></pre>
<p><img src="knn_files/figure-html/unnamed-chunk-10-2.png" width="672" /></p>
<pre class="r"><code>coords(roc1, &quot;best&quot;, ret=c(&quot;threshold&quot;, &quot;specificity&quot;, &quot;1-npv&quot;))</code></pre>
<pre><code>## Warning in coords.roc(roc1, &quot;best&quot;, ret = c(&quot;threshold&quot;, &quot;specificity&quot;, : The
## &#39;transpose&#39; argument to FALSE by default since pROC 1.16. Set transpose = TRUE
## explicitly to revert to the previous behavior, or transpose = TRUE to silence
## this warning. Type help(coords_transpose) for additional information.</code></pre>
<pre><code>##            threshold specificity    1-npv
## threshold 0.03488372    74.93113 0.729927</code></pre>
<hr />
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
