<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>PCA</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">ML</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="Cluster.html">Clustering</a>
</li>
<li>
  <a href="PCA.html">PCA</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Clasificadores
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="GAMLogit.html">GAM</a>
    </li>
    <li>
      <a href="glm.html">GLM</a>
    </li>
    <li>
      <a href="knn.html">KNN</a>
    </li>
    <li>
      <a href="decision_trees.html">Decision Trees</a>
    </li>
    <li>
      <a href="random_forest.html">Random Forest</a>
    </li>
    <li>
      <a href="svm.html">SVM</a>
    </li>
  </ul>
</li>
<li>
  <a href="GAM.html">GAM - regresión</a>
</li>
<li>
  <a href="evaluacion.html">Evaluación</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">PCA</h1>

</div>


<p>Intentaremos hacer una descomposición PCA de los datos del dataset para ver si es viable en nuestro dataset simplificar el número de variables a utilizar en los modelos (sobre todos en modelos que no son explicativos, tal y como SVM, y en los que ayuda esta reducción de dimensionalidad en los recursos computacionales utilizados por dicho modelo). Como tenemos variables de todo tipo (discretas, contínuas, categóricas, ordinales) utilizarmos una librería denominada PCAmixdata que es capaz de generar esta descomposición en valores principales mezclando todos los tipos de variable. Para ello, previamente hay que descomponer el dataset en variables de tipo cualitativas y cuantitativas. Para ello la propia librería tiene una función que automatiza esta tarea. El siguiente código realiza esta descomposición, tanto en el dataset completo como únicamente en los datos del dataset de Train:</p>
<pre class="r"><code>library(PCAmixdata)
splitCompleto &lt;- splitmix(dataSetTrainPCA)
X1SplitCompleto &lt;- splitCompleto$X.quanti 
X2SplitCompleto &lt;- splitCompleto$X.quali 
objetoPCAMIXCompleto &lt;- PCAmix(X.quanti=X1SplitCompleto, X.quali=X2SplitCompleto,rename.level=TRUE, graph=FALSE,ndim=60)</code></pre>
<p>Mostramos un grafico con el incremento de varianza explicada por cada uno de los autoectores obtenidos en el anterior proceso:</p>
<pre class="r"><code>plot(objetoPCAMIXCompleto$eig[,3], xlab = &#39;Numero de Autovalor&#39;, ylab = &#39;Varianza Acumulada&#39;, main = &#39;Varianza cumulada por cada uno de los eigenvalues&#39;)
lines(objetoPCAMIXCompleto$eig[,3])</code></pre>
<p><img src="PCA_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Como vemos, con el 60% de las variables obtenemos una varianza acumulado del 80%. No obstante, si vemos el número de dimenosiones, vemos que para calcular este tipo de PCA se ha realizado un “one hot” encoding de las variables categóricas, y esto para alguno de los modelos (random forest, knn) no es conveniente. No obstante, para el modelo basado en SVM se podría probar a ver si con esta reducción de la dimensionalidad conseguimos resultados equivalentes. EN los modelos GLM y GAM, preferimos no utilizar este tipo de codificación debido a que se pierde interpretabilildad en el modelo.</p>
<p>Como ejemplo del uso de estas técnicas en un modelo de ML, entrenaremos un RF rápido para comprobar si la reducción de la dimensionalidad conlleva un modelo con similares poderes de predicción:</p>
<pre class="r"><code>numeroDimOptimo = 60

 

splitTest &lt;- splitmix(datasetTestPCA%&gt;% dplyr::select(-c(GrupoPrecio)))
X1Test &lt;- splitTest$X.quanti 
X2Test &lt;- splitTest$X.quali 

#objetoPCAMIXTrain$eig
coordenadasPCATrain=as.data.table(cbind(objetoPCAMIXCompleto$ind$coord, GrupoPrecio= datasetTrainPCA %&gt;% dplyr::select(c(&quot;GrupoPrecio&quot;))  ))

coordenadasPCATest=as.data.table(cbind(predict(objetoPCAMIXCompleto,X1Test,X2Test), GrupoPrecio=datasetTestPCA %&gt;% dplyr::select(c(&quot;GrupoPrecio&quot;)) ))

# ahora la regresion por random forest con los componentes PCA

library(caret)

ctrl = trainControl(method=&quot;repeatedcv&quot;,
                    number=2,
                    repeats=1)

tGrid &lt;-  expand.grid(mtry = c(7))

rf_model_pca &lt;- train(GrupoPrecio ~.,
                data=coordenadasPCATrain,          
                method=&quot;rf&quot;,
                nodesize= 30,
                ntree =500,
                do.trace= 10,
                trControl=ctrl,
                tuneGrid = tGrid,
                verbose = FALSE
                 )</code></pre>
<pre><code>## ntree      OOB      1      2
##    10:   7.92%  0.95% 91.43%
##    20:   8.27%  1.65% 88.57%
##    30:   8.16%  1.41% 90.00%
##    40:   7.62%  0.59% 92.86%
##    50:   7.62%  0.82% 90.00%
##    60:   7.83%  0.82% 92.86%
##    70:   7.83%  0.82% 92.86%
##    80:   7.62%  0.82% 90.00%
##    90:   7.62%  0.82% 90.00%
##   100:   7.94%  0.94% 92.86%
##   110:   7.94%  0.71% 95.71%
##   120:   7.83%  0.59% 95.71%
##   130:   7.83%  0.59% 95.71%
##   140:   7.62%  0.35% 95.71%
##   150:   7.62%  0.35% 95.71%
##   160:   7.83%  0.47% 97.14%
##   170:   8.05%  0.59% 98.57%
##   180:   7.73%  0.47% 95.71%
##   190:   7.83%  0.59% 95.71%
##   200:   7.83%  0.47% 97.14%
##   210:   7.73%  0.47% 95.71%
##   220:   7.73%  0.47% 95.71%
##   230:   8.05%  0.59% 98.57%
##   240:   8.05%  0.59% 98.57%
##   250:   7.94%  0.47% 98.57%
##   260:   8.05%  0.59% 98.57%
##   270:   8.05%  0.59% 98.57%
##   280:   8.05%  0.59% 98.57%
##   290:   8.05%  0.59% 98.57%
##   300:   8.05%  0.59% 98.57%
##   310:   8.05%  0.59% 98.57%
##   320:   8.16%  0.59%100.00%
##   330:   8.16%  0.59%100.00%
##   340:   8.16%  0.59%100.00%
##   350:   7.94%  0.35%100.00%
##   360:   8.16%  0.59%100.00%
##   370:   8.16%  0.59%100.00%
##   380:   7.94%  0.47% 98.57%
##   390:   8.05%  0.59% 98.57%
##   400:   8.05%  0.59% 98.57%
##   410:   8.16%  0.59%100.00%
##   420:   8.16%  0.59%100.00%
##   430:   8.05%  0.47%100.00%
##   440:   8.05%  0.47%100.00%
##   450:   8.05%  0.47%100.00%
##   460:   7.94%  0.47% 98.57%
##   470:   8.16%  0.59%100.00%
##   480:   8.16%  0.59%100.00%
##   490:   8.16%  0.59%100.00%
##   500:   8.16%  0.59%100.00%
## ntree      OOB      1      2
##    10:   8.55%  2.26% 84.29%
##    20:   7.73%  1.18% 87.14%
##    30:   7.52%  0.94% 87.14%
##    40:   7.52%  0.83% 88.57%
##    50:   7.41%  0.59% 90.00%
##    60:   7.41%  0.47% 91.43%
##    70:   7.41%  0.59% 90.00%
##    80:   7.52%  0.47% 92.86%
##    90:   7.41%  0.35% 92.86%
##   100:   7.41%  0.35% 92.86%
##   110:   7.30%  0.35% 91.43%
##   120:   7.41%  0.35% 92.86%
##   130:   7.30%  0.35% 91.43%
##   140:   7.30%  0.35% 91.43%
##   150:   7.41%  0.35% 92.86%
##   160:   7.52%  0.35% 94.29%
##   170:   7.41%  0.35% 92.86%
##   180:   7.52%  0.35% 94.29%
##   190:   7.30%  0.24% 92.86%
##   200:   7.30%  0.24% 92.86%
##   210:   7.52%  0.35% 94.29%
##   220:   7.41%  0.35% 92.86%
##   230:   7.19%  0.35% 90.00%
##   240:   7.08%  0.24% 90.00%
##   250:   7.08%  0.24% 90.00%
##   260:   7.08%  0.12% 91.43%
##   270:   7.08%  0.12% 91.43%
##   280:   7.08%  0.12% 91.43%
##   290:   6.97%  0.00% 91.43%
##   300:   7.08%  0.12% 91.43%
##   310:   7.08%  0.12% 91.43%
##   320:   7.08%  0.12% 91.43%
##   330:   7.08%  0.12% 91.43%
##   340:   7.08%  0.12% 91.43%
##   350:   7.08%  0.12% 91.43%
##   360:   7.08%  0.12% 91.43%
##   370:   7.08%  0.12% 91.43%
##   380:   7.08%  0.12% 91.43%
##   390:   7.08%  0.12% 91.43%
##   400:   7.19%  0.12% 92.86%
##   410:   7.19%  0.12% 92.86%
##   420:   7.08%  0.12% 91.43%
##   430:   7.08%  0.12% 91.43%
##   440:   7.08%  0.12% 91.43%
##   450:   7.19%  0.12% 92.86%
##   460:   7.08%  0.12% 91.43%
##   470:   7.19%  0.12% 92.86%
##   480:   7.19%  0.12% 92.86%
##   490:   7.19%  0.12% 92.86%
##   500:   7.19%  0.12% 92.86%
## ntree      OOB      1      2
##    10:   8.07%  2.14% 79.86%
##    20:   7.57%  1.65% 79.29%
##    30:   7.24%  1.00% 82.86%
##    40:   6.86%  0.88% 79.29%
##    50:   7.08%  1.06% 80.00%
##    60:   6.86%  1.00% 77.86%
##    70:   6.91%  1.00% 78.57%
##    80:   7.19%  0.88% 83.57%
##    90:   7.24%  0.88% 84.29%
##   100:   7.19%  0.82% 84.29%
##   110:   7.19%  0.77% 85.00%
##   120:   7.35%  0.77% 87.14%
##   130:   7.29%  0.71% 87.14%
##   140:   7.51%  0.94% 87.14%
##   150:   7.35%  0.77% 87.14%
##   160:   7.35%  0.82% 86.43%
##   170:   7.51%  0.82% 88.57%
##   180:   7.68%  0.82% 90.71%
##   190:   7.62%  0.88% 89.29%
##   200:   7.51%  0.77% 89.29%
##   210:   7.46%  0.77% 88.57%
##   220:   7.40%  0.71% 88.57%
##   230:   7.51%  0.71% 90.00%
##   240:   7.35%  0.65% 88.57%
##   250:   7.24%  0.59% 87.86%
##   260:   7.46%  0.71% 89.29%
##   270:   7.29%  0.59% 88.57%
##   280:   7.24%  0.59% 87.86%
##   290:   7.35%  0.53% 90.00%
##   300:   7.35%  0.53% 90.00%
##   310:   7.46%  0.65% 90.00%
##   320:   7.40%  0.59% 90.00%
##   330:   7.40%  0.59% 90.00%
##   340:   7.40%  0.65% 89.29%
##   350:   7.40%  0.59% 90.00%
##   360:   7.35%  0.59% 89.29%
##   370:   7.46%  0.65% 90.00%
##   380:   7.46%  0.65% 90.00%
##   390:   7.51%  0.65% 90.71%
##   400:   7.40%  0.65% 89.29%
##   410:   7.35%  0.59% 89.29%
##   420:   7.46%  0.65% 90.00%
##   430:   7.46%  0.65% 90.00%
##   440:   7.40%  0.59% 90.00%
##   450:   7.40%  0.59% 90.00%
##   460:   7.29%  0.47% 90.00%
##   470:   7.19%  0.35% 90.00%
##   480:   7.40%  0.59% 90.00%
##   490:   7.29%  0.47% 90.00%
##   500:   7.35%  0.53% 90.00%</code></pre>
<pre class="r"><code># y por último , genero el mismo random forest con todos los componenes
rf_model_sin_pca &lt;-train(GrupoPrecio ~.,
                data=datasetTrainPCA,          
                method=&quot;rf&quot;,
                nodesize= 30,
                ntree =500,
                do.trace= 10,
                trControl=ctrl,
                tuneGrid = tGrid,
                verbose = FALSE,
                 )</code></pre>
<pre><code>## ntree      OOB      1      2
##    10:   5.29%  0.95% 60.61%
##    20:   6.31%  0.71% 74.29%
##    30:   6.20%  0.59% 74.29%
##    40:   6.09%  0.47% 74.29%
##    50:   5.33%  0.24% 67.14%
##    60:   4.90%  0.24% 61.43%
##    70:   5.22%  0.24% 65.71%
##    80:   5.22%  0.12% 67.14%
##    90:   5.44%  0.24% 68.57%
##   100:   5.11%  0.24% 64.29%
##   110:   4.79%  0.12% 61.43%
##   120:   5.01%  0.12% 64.29%
##   130:   4.90%  0.12% 62.86%
##   140:   5.01%  0.24% 62.86%
##   150:   5.11%  0.00% 67.14%
##   160:   5.33%  0.00% 70.00%
##   170:   5.22%  0.00% 68.57%
##   180:   5.22%  0.00% 68.57%
##   190:   5.44%  0.00% 71.43%
##   200:   5.44%  0.00% 71.43%
##   210:   5.44%  0.00% 71.43%
##   220:   5.33%  0.00% 70.00%
##   230:   5.22%  0.00% 68.57%
##   240:   5.11%  0.00% 67.14%
##   250:   5.01%  0.00% 65.71%
##   260:   5.01%  0.00% 65.71%
##   270:   4.90%  0.00% 64.29%
##   280:   5.01%  0.00% 65.71%
##   290:   5.01%  0.00% 65.71%
##   300:   4.90%  0.00% 64.29%
##   310:   4.90%  0.00% 64.29%
##   320:   4.90%  0.00% 64.29%
##   330:   5.11%  0.00% 67.14%
##   340:   5.11%  0.00% 67.14%
##   350:   5.11%  0.00% 67.14%
##   360:   4.90%  0.00% 64.29%
##   370:   5.01%  0.00% 65.71%
##   380:   5.11%  0.00% 67.14%
##   390:   4.90%  0.00% 64.29%
##   400:   5.01%  0.00% 65.71%
##   410:   4.90%  0.00% 64.29%
##   420:   5.11%  0.00% 67.14%
##   430:   5.11%  0.00% 67.14%
##   440:   5.01%  0.00% 65.71%
##   450:   4.79%  0.00% 62.86%
##   460:   4.90%  0.00% 64.29%
##   470:   4.90%  0.00% 64.29%
##   480:   4.90%  0.00% 64.29%
##   490:   4.90%  0.00% 64.29%
##   500:   4.90%  0.00% 64.29%
## ntree      OOB      1      2
##    10:   4.41%  0.59% 52.24%
##    20:   6.10%  0.35% 75.71%
##    30:   6.43%  0.12% 82.86%
##    40:   5.99%  0.00% 78.57%
##    50:   5.34%  0.12% 68.57%
##    60:   5.01%  0.00% 65.71%
##    70:   4.14%  0.00% 54.29%
##    80:   4.68%  0.00% 61.43%
##    90:   4.58%  0.12% 58.57%
##   100:   5.23%  0.12% 67.14%
##   110:   4.79%  0.12% 61.43%
##   120:   4.79%  0.12% 61.43%
##   130:   4.58%  0.12% 58.57%
##   140:   4.58%  0.12% 58.57%
##   150:   4.90%  0.12% 62.86%
##   160:   4.90%  0.12% 62.86%
##   170:   4.90%  0.12% 62.86%
##   180:   5.01%  0.12% 64.29%
##   190:   5.23%  0.12% 67.14%
##   200:   4.79%  0.12% 61.43%
##   210:   4.90%  0.12% 62.86%
##   220:   4.68%  0.12% 60.00%
##   230:   5.01%  0.12% 64.29%
##   240:   4.79%  0.12% 61.43%
##   250:   4.79%  0.12% 61.43%
##   260:   4.58%  0.00% 60.00%
##   270:   4.79%  0.00% 62.86%
##   280:   4.79%  0.00% 62.86%
##   290:   4.79%  0.00% 62.86%
##   300:   5.12%  0.12% 65.71%
##   310:   5.12%  0.12% 65.71%
##   320:   5.23%  0.12% 67.14%
##   330:   5.23%  0.12% 67.14%
##   340:   5.34%  0.12% 68.57%
##   350:   5.45%  0.12% 70.00%
##   360:   5.45%  0.12% 70.00%
##   370:   5.45%  0.12% 70.00%
##   380:   5.56%  0.12% 71.43%
##   390:   5.45%  0.12% 70.00%
##   400:   5.45%  0.12% 70.00%
##   410:   5.45%  0.12% 70.00%
##   420:   5.45%  0.12% 70.00%
##   430:   5.34%  0.00% 70.00%
##   440:   5.34%  0.00% 70.00%
##   450:   5.45%  0.12% 70.00%
##   460:   5.34%  0.00% 70.00%
##   470:   5.23%  0.00% 68.57%
##   480:   5.45%  0.00% 71.43%
##   490:   5.56%  0.12% 71.43%
##   500:   5.56%  0.12% 71.43%
## ntree      OOB      1      2
##    10:   4.23%  0.83% 45.32%
##    20:   3.81%  0.59% 42.86%
##    30:   2.78%  0.24% 33.57%
##    40:   3.43%  0.24% 42.14%
##    50:   2.99%  0.00% 39.29%
##    60:   2.40%  0.06% 30.71%
##    70:   2.72%  0.06% 35.00%
##    80:   1.96%  0.06% 25.00%
##    90:   1.74%  0.06% 22.14%
##   100:   1.80%  0.00% 23.57%
##   110:   2.12%  0.06% 27.14%
##   120:   2.01%  0.06% 25.71%
##   130:   2.12%  0.00% 27.86%
##   140:   2.12%  0.00% 27.86%
##   150:   1.85%  0.00% 24.29%
##   160:   1.85%  0.00% 24.29%
##   170:   2.01%  0.00% 26.43%
##   180:   2.18%  0.00% 28.57%
##   190:   2.23%  0.00% 29.29%
##   200:   2.50%  0.00% 32.86%
##   210:   2.67%  0.00% 35.00%
##   220:   2.50%  0.00% 32.86%
##   230:   2.50%  0.00% 32.86%
##   240:   2.67%  0.00% 35.00%
##   250:   2.56%  0.00% 33.57%
##   260:   2.45%  0.00% 32.14%
##   270:   2.50%  0.00% 32.86%
##   280:   2.45%  0.00% 32.14%
##   290:   2.34%  0.00% 30.71%
##   300:   2.61%  0.00% 34.29%
##   310:   2.56%  0.00% 33.57%
##   320:   2.61%  0.00% 34.29%
##   330:   2.61%  0.00% 34.29%
##   340:   2.72%  0.00% 35.71%
##   350:   2.61%  0.00% 34.29%
##   360:   2.67%  0.00% 35.00%
##   370:   2.72%  0.00% 35.71%
##   380:   2.72%  0.00% 35.71%
##   390:   2.61%  0.00% 34.29%
##   400:   2.50%  0.00% 32.86%
##   410:   2.45%  0.00% 32.14%
##   420:   2.34%  0.00% 30.71%
##   430:   2.40%  0.00% 31.43%
##   440:   2.34%  0.00% 30.71%
##   450:   2.34%  0.00% 30.71%
##   460:   2.29%  0.00% 30.00%
##   470:   2.34%  0.00% 30.71%
##   480:   2.50%  0.00% 32.86%
##   490:   2.67%  0.00% 35.00%
##   500:   2.45%  0.00% 32.14%</code></pre>
<pre class="r"><code>rf_model_pca$results</code></pre>
<pre><code>##   mtry  Accuracy      Kappa  AccuracySD    KappaSD
## 1    7 0.9243322 0.09771624 0.002367801 0.07651823</code></pre>
<pre class="r"><code>rf_model_sin_pca$results</code></pre>
<pre><code>##   mtry  Accuracy     Kappa AccuracySD   KappaSD
## 1    7 0.9488254 0.4673004 0.01081729 0.1605745</code></pre>
<p>Comprobamos que el accuracy en ambos modelos es similar con un 40% de reducción en el número de variables.</p>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
