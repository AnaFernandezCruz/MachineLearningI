---
title: "RANDOM FOREST"
---


En este punto vamos a aplicar los bosques de árboles  a nuestro modelo. Ya conocemos el funcionamiento de los árboles de decisión por lo que nos será fácil entender el sistema de funcionamiento de los bosques: es un conjunto de árboles con distintos parámetros y mediante un sistema de votación de los resultados de los árboles obtendremos el resultado del bosque.

Para empezar, cargamos nuestros datos.

```{r message = FALSE, warning=FALSE}
library(class)
library(dplyr)
library(caret)
library (ROCR)
library(MASS)
library(DMwR)
library(hmeasure)
library(data.table)

dataTrain_origin <- readRDS("datasetTrainModeloClasificador.rds")
dataTest_origin <- readRDS("datasetTestModeloClasificador.rds")

dataTrain_origin_PCA <- readRDS("datasetTrainModeloClasificadorPCA.rds")
dataTest_origin_PCA <- readRDS("datasetTestModeloClasificadorPCA.rds") 

PCATrain <- as.data.table(cbind(dataTrain_origin_PCA$ind$coord, GrupoPrecio = dataTrain_origin %>% dplyr::select(c("GrupoPrecio"))))
PCATest <- as.data.table(cbind(dataTest_origin_PCA$ind$coord, GrupoPrecio = dataTest_origin %>% dplyr::select(c("GrupoPrecio"))))

XPCATrain <- PCATrain %>% dplyr::select(-GrupoPrecio)
YPCATrain <- PCATrain$GrupoPrecio
XPCATest <- PCATest %>% dplyr::select(-GrupoPrecio)
YPCATest <- PCATest$GrupoPrecio
```


Vamos a afrontar el problema usando random forest. Este modelo no lo podemos comprender previamente como hemos hecho con el árbol de decisión, así que tenemos que ajustarlo de manera correcta para obtener los mejores resultados. En este apartado nos centraremos en el ajuste de dos hiperparámetros: mtry y ntree. 

 - mtry: el número de variables que cogemos para realizar cada muestreo.

 - ntree: el número de ramas.

En el código que tenemos aqui, utilizamos el método repeatedcv de la herramientra trainControl de Caret y dividiremos el conjunto de datos en 10 grupos para hacer validación cruzada y repetiremos 3 veces para ralentizar nuestro proceso. 


```{r message = FALSE, warning=FALSE}
library(caret)

#10 folds repeat 3 times
controlrf <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3,
                        classProbs = TRUE, 
                        summaryFunction = twoClassSummary,
                        savePredictions = TRUE,
                        sampling = "smote")
#Metric compare model is Accuracy
metric <- "Accuracy"
set.seed(123)
#Number randomely variable selected is mtry
mtry <- sqrt(ncol(PCATrain))
tunegridrf <- expand.grid(.mtry=mtry)
rf_default <- train(GrupoPrecio~., 
                      data=PCATrain, 
                      method='rf', 
                      metric='Accuracy', 
                      tuneGrid=tunegridrf, 
                      trControl=controlrf)

print(rf_default)
```

Si evaluamos este modelo, tenemos la siguiente información:

```{r message = FALSE, warning=FALSE}
##MATRIZ CONFUSION
house.pred_default <- predict(rf_default, newdata = PCATest)
table_random <- table(house.pred_default, PCATest$GrupoPrecio)
table_random


cm_random <- confusionMatrix(house.pred_default, PCATest$GrupoPrecio, mode = "prec_recall")
cm_random

##CÁLCULO DEL ERROR
error.rate_random = round(mean(house.pred_default != PCATest$GrupoPrecio), 2)
error.rate_random
```


La biblioteca Caret nos proporciona herramientas para declarar de manera aleatoria las variables. El código de aqui genera aleatoriamente 10 modelos con distintos valores de mtry. 

```{r message = FALSE, warning=FALSE}
control2 <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3,
                        search = 'random',
                        sampling = "smote")

#Random generate 15 mtry values with tuneLength = 15
set.seed(1)
rf_random <- train(GrupoPrecio ~ .,
                   data = PCATrain,
                   method = 'rf',
                   metric = 'Accuracy',
                   tuneLength  = 10, 
                   trControl = control2)
print(rf_random)
plot(rf_random)
```



Como resultado obtenemos que para mtry = 1 el accuracy es más alto.

También podemos hacer un Grid Search para buscar los parámetros mejores para nuestro modelo, en nuestro caso el número de árboles. 

```{r message = FALSE, warning=FALSE}
bestMtry <- 1
control1 <- trainControl(method = 'repeatedcv',number = 5, sampling = "smote")
                               

storeMaxtrees <- list()
tuneGrid1 <- expand.grid(.mtry = bestMtry)
for (ntree in c(300, 500, 700, 1000, 1500, 2000)) {
  set.seed(291)
  rf.maxtrees <- train(GrupoPrecio ~ .,
                       data = PCATrain,
                       method = "rf",
                       metric = "Accuracy",
                       tuneGrid = tuneGrid1,
                       trControl = control1,
                       importance = TRUE,
                       nodesize = 14,
                       maxnodes = 24,
                       ntree = ntree)
  key <- toString(ntree)
  storeMaxtrees[[key]] <- rf.maxtrees
}
resultsTree <- resamples(storeMaxtrees)

res = summary(resultsTree)
print(res)
res$models[which.max(res$statistics$Accuracy[,"Mean"])]

```
Y obtenemos que el número de árboles recomendado es de 700 árboles.

Si entrenenamos el modelo con nuestros parámetros:

```{r message = FALSE, warning=FALSE}
model_control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3,
                        savePredictions = TRUE,
                        classProbs = TRUE, 
                        summaryFunction = twoClassSummary,
                        sampling = "smote")

set.seed(123)

model_tunegrid <- expand.grid(.mtry= 1)
rf_model <- train(GrupoPrecio ~ .,
                       data = PCATrain,
                       method = "rf",
                       metric = "Accuracy",
                       tuneGrid = model_tunegrid,
                       trControl = model_control,
                       importance = TRUE,
                       nodesize = 14,
                       maxnodes = 24,
                       ntree = 700)

print(rf_model)
model_pca <- rf_model
```

Evaluamos los resultados:
```{r message = FALSE, warning=FALSE}
##MATRIZ CONFUSION
house.predmodel_pca_ <- predict(model_pca, newdata = PCATest)
table_rf <- table(house.predmodel_pca_, PCATest$GrupoPrecio)
table_rf


cm_rf_pca <- confusionMatrix(house.predmodel_pca_, PCATest$GrupoPrecio, mode = "prec_recall")
cm_rf_pca

##CÁLCULO DEL ERROR
error.rate_rf_pca = mean(house.predmodel_pca_ != PCATest$GrupoPrecio)
error.rate_rf_pca
```


Otra de las pruebas que podemos hacer es calcular los hiperparámetros para el dataset sin PCA. Para ello cargamos los dataset sin PCA y realizamos el mismo trabajo.


```{r echo = FALSE}
dataTrain <- readRDS("datasetTrainModeloClasificador.rds") 
myvars <- names(dataTrain) %in% c('SalePrice')
dataTrain <- dataTrain[!myvars]

dataTest <- readRDS("datasetTestModeloClasificador.rds")
myvars <- names(dataTest) %in% c('SalePrice')
dataTest <- dataTest[!myvars]
```

Calculamos el número óptimo de mtry: 

```{r message = FALSE, warning=FALSE}
control2 <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3,
                        search = 'random',
                        sampling = "smote")

#Random generate 15 mtry values with tuneLength = 15
set.seed(1)
dataTrain_rf_random <- train(GrupoPrecio ~ .,
                   data = dataTrain,
                   method = 'rf',
                   metric = 'Accuracy',
                   tuneLength  = 10, 
                   trControl = control2)
print(dataTrain_rf_random)
plot(dataTrain_rf_random)
```
Sorprendentemente, vemos que en este caso tenemos dos puntos que predominan en el accuracy: 1 y 50. 

```{r message = FALSE, warning=FALSE}
bestMtry <- 50
control1 <- trainControl(method = 'repeatedcv',number = 5, sampling = "smote")
                               

storeMaxtrees <- list()
tuneGrid1 <- expand.grid(.mtry = bestMtry)
for (ntree in c(1000, 1500, 2000)) {
  set.seed(291)
  rf.maxtrees <- train(GrupoPrecio ~ .,
                       data = dataTrain,
                       method = "rf",
                       metric = "Accuracy",
                       tuneGrid = tuneGrid1,
                       trControl = control1,
                       importance = TRUE,
                       nodesize = 14,
                       maxnodes = 24,
                       ntree = ntree)
  key <- toString(ntree)
  storeMaxtrees[[key]] <- rf.maxtrees
}
resultsTree2 <- resamples(storeMaxtrees)

res2 = summary(resultsTree2)
print(res)
res2$models[which.max(res2$statistics$Accuracy[,"Mean"])]

```
El modelo nos devuelve que la cantidad de árboles necesaria es de 1500.

Entrenamos nuestro modelo con estos hiperparámetros:


```{r message = FALSE, warning=FALSE}
model_control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3,
                        savePredictions = TRUE,
                        classProbs = TRUE, 
                        summaryFunction = twoClassSummary,
                        sampling = "smote")

set.seed(123)

model_tunegrid <- expand.grid(.mtry= 50)
rf_model_train <- train(GrupoPrecio ~ .,
                       data = PCATrain,
                       method = "rf",
                       metric = "Accuracy",
                       tuneGrid = model_tunegrid,
                       trControl = model_control,
                       importance = TRUE,
                       nodesize = 14,
                       maxnodes = 24,
                       ntree = 1500)

print(rf_model_train)
model_nopca <- rf_model_train
```

Evaluamos los resultados:
```{r message = FALSE, warning=FALSE}
##MATRIZ CONFUSION
house.predmodel_nopca <- predict(model_nopca, newdata = PCATest)
table_rf <- table(house.pred_rf_nopca, dataTest$GrupoPrecio)
table_rf


cm_rf_nopca <- confusionMatrix(house.predmodel_nopca, dataTest$GrupoPrecio, mode = "prec_recall")
cm_rf_nopca

##CÁLCULO DEL ERROR
error.rate_rf_nopca = mean(house.predmodel_nopca  != dataTest$GrupoPrecio)
error.rate_rf_nopca
```


Para comparar los modelos podemos acudir a sus matrices de confusiones y errores además de a la curva ROC de cada modelo:
```{r}
### PCA
cm_rf_pca
error.rate_rf_pca

print('------------')

### NO PCA
cm_rf_nopca
error.rate_rf_nopca
```

En este caso vemos que si usamos el dataset en el que NO hemos aplicado PCA el modelo sale considerablemente mejor si nos fijamos en la matriz de confusión, aunque podemos ver que tiene un error un poco más alto. 

```{r , message=FALSE, warning=FALSE, echo=TRUE}
library(MLeval)
res <- evalm(list(model_pca, model_nopca),gnames = c('model_pca', 'model_nopca'))
```

```{r}
saveRDS(model_pca, "randomforest_pca.rds")
saveRDS(model_nopca, "randomforest_nopca.rds")
```