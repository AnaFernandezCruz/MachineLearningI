---
title: "GAM"
output: 
   html_document:
      code_folding: hide
---

## R Markdown

En primer lugar cargaremos los datasets obtenidos en las fases anteriores del modelado de datos.

```{r  message=FALSE, warning=FALSE, error = FALSE , results="hide"}

library(mgcv)
library(car)
library(parallel)
library(dplyr)
library(randomForest)


datasetTrain <-  readRDS("datasetTrain.rds")
datasetTest<-  readRDS("datasetTest.rds")
validation_sin_na<-  readRDS("datasetValidation.rds")
datasetTrainTransformed<-  readRDS("datasetTrainTransformed.rds")
datasetTestTransformed<-  readRDS("datasetTestTransformed.rds")
validationTransformed<-  readRDS("validationTransformed.rds")


#names(datasetTrain) <- make.names(names(datasetTrain))
#names(datasetTest) <- make.names(names(datasetTrain))
#names(validation_sin_na) <- make.names(names(validation_sin_na))
#names(datasetTrainTransformed) <- make.names(names(datasetTrainTransformed))
#names(datasetTestTransformed) <- make.names(names(datasetTestTransformed))
#names(validationTransformed) <- make.names(names(validation_sin_na))

dataSetTrainGAMLogistic <-  readRDS('datasetTrainModeloClasificador.rds')   %>% filter( ! (exp(SalePrice) < 200000 &&exp(GrLivArea) > 4000  ))  %>% dplyr::select(-c('SalePrice'))  %>% na.omit()

dataSetTestGAMLogistic <- readRDS('datasetTestModeloClasificador.rds')   %>% filter(! (exp(SalePrice) < 200000 &&exp(GrLivArea) > 4000  ))  %>% dplyr::select(-c('SalePrice'))  %>% na.omit()
names(dataSetTrainGAMLogistic) <- make.names(names(dataSetTrainGAMLogistic))
names(dataSetTestGAMLogistic) <- make.names(names(dataSetTestGAMLogistic))

variablesEliminarCorr <- c()

invisible({capture.output({

    detectCores()
     
    ###indicate number of cores used for parallel processing
    if (detectCores()>1) {
    cl <- makeCluster(detectCores()-1)
    } else cl <- NULL
    
})})    

```

Quitaremos de estos datasets posibles outliers que se hayan podido colar en pasos anteriores del procesamiento (casas muy baratos on una superficie exageradamente alta) y dividiremos en dataset en casas "baratas" (menos de 300000 dólares) y casas "caras" (más de 300000 dólares). Estos dataset serviran para extaer las variables más influyentes dentro de las poblaciones y crear a posteriori una regresión logística de tipo GAM para intentar discriminar casas caras de las baratas:

```{r  message=FALSE, warning=FALSE}

dataSetTrainGAM = datasetTrain %>%   filter( ! (exp(SalePrice) < 200000 &&exp(GrLivArea) > 4000  ))  %>%  na.omit()
dataSetTestGAM = datasetTest %>% filter( ! (exp(SalePrice) < 200000 &&exp(GrLivArea) > 4000  ))  %>%  na.omit()
dataSetValidationGAM = validation_sin_na  %>%  na.omit()

names(dataSetTrainGAM) <- make.names(names(dataSetTrainGAM))
names(dataSetTestGAM) <- make.names(names(dataSetTestGAM))
names(dataSetValidationGAM) <- make.names(names(dataSetValidationGAM))


dataSetTrainGAMCaros <-  dataSetTrainGAM %>% filter(exp(SalePrice) > 250000)
dataSetTrainGAMBaratos <- dataSetTrainGAM %>% filter(exp(SalePrice) < 250000)
dataSetTestGAMCaros <-  dataSetTestGAM %>% filter(exp(SalePrice) > 250000)
dataSetTestGAMBaratos <- dataSetTestGAM %>% filter(exp(SalePrice) < 250000)


```

  La idea de esta práctica es generar dos modelos y compararlos entre sí:
   - Un primer modelo GAM de regresión con la totalidad de la población y las variables más importantes de cara a la exactitud de la predicción.
   - Crearemos dos modelos GAM de regresión, uno con las casas categorizadas como baratas, otro con las casas categorizadas como caras, y por último, utilizando el clasificador del paso anterior, el resultado final del modelo será el precio de la casa según el modelo de casas baratas multiplicado por la probabilidad de que la casa sea barata al que sumaremos el precio de la casa cara multiplicado por el precio de la casa cara. 
    


```{r  message=FALSE, warning=FALSE}
library(ggplot2)

dataSetTrainTestGAMBaratos <- dplyr::union(dataSetTrainGAMBaratos,dataSetTestGAMBaratos) 
dataSetTrainTestGAMCaros   <- dplyr::union(dataSetTrainGAMCaros,dataSetTestGAMCaros) 
dataSetTrainTestGAM <- dplyr::union(dataSetTrainGAM,dataSetTestGAM) 

excludedVars <- c('SalePrice','GrupoPrecio')

set.seed(2018)
arbol_visualizacion <- randomForest(x= dataSetTrainTestGAM %>% dplyr::select(-excludedVars), y=dataSetTrainTestGAM$SalePrice, ntree=100,importance=TRUE)
varImportantes <- importance(arbol_visualizacion)
dataframeTestImportantes <- data.frame(Variables = row.names(varImportantes), MSE = varImportantes[,1])
dataframeTestImportantes <- dataframeTestImportantes[order(dataframeTestImportantes$MSE, decreasing = TRUE),]


```

 
```{r  message=FALSE, warning=FALSE, echo=FALSE}
arbol_visualizacion <- randomForest(x=dataSetTrainTestGAMBaratos %>% dplyr::select(-excludedVars ), y=dataSetTrainTestGAMBaratos$SalePrice, ntree=100,importance=TRUE)
varImportantesTest <- importance(arbol_visualizacion)
dataframeTestImportantesBaratos <- data.frame(Variables = row.names(varImportantesTest), MSE = varImportantesTest[,1])
dataframeTestImportantesBaratos <- dataframeTestImportantesBaratos[order(dataframeTestImportantesBaratos$MSE, decreasing = TRUE),]



```

 
```{r  message=FALSE, warning=FALSE, echo=FALSE}

arbol_visualizacion <- randomForest(x=dataSetTrainTestGAMCaros %>% dplyr::select(-excludedVars), y=dataSetTrainTestGAMCaros$SalePrice, ntree=100,importance=TRUE)
varImportantesTest <- importance(arbol_visualizacion)
dataframeTestImportantesCaros <- data.frame(Variables = row.names(varImportantesTest), MSE = varImportantesTest[,1])
dataframeTestImportantesCaros <- dataframeTestImportantesCaros[order(dataframeTestImportantesCaros$MSE, decreasing = TRUE),]

```
   


```{r  message=FALSE, warning=FALSE, echo=FALSE}

var_modelo = c( 'GrLivArea','LotArea','Total_Bathrooms','MSZoning','LandContour','LotConfig','Condition1','BldgType','HouseStyle','RoofStyle','MasVnrType','Foundation','SaleType','SaleCondition','YearBuilt','OverallQual','OverallCond','PavedDrive','Fence','BsmtFinSF1','ExterQual','BsmtQual','BsmtExposure','BsmtFinType1','BsmtUnfSF','CentralAir','KitchenQual','Fireplaces','FireplaceQu','BedroomAbvGr','KitchenAbvGr','LotShape','GarageArea','GarageCond','Neighborhood','TotalSF','GarageType','MSSubClass','MasVnrArea','Total_porch_SF')

var_modelo_with_SalePrice = c(var_modelo,'SalePrice')

variablesEliminarCorr = c('TotalSF','X2ndFlrSF','FullBath','BsmtFullBath','X1stFlrSF')

```

  A continuación se muestran una serie de funciones de apoyo que ayudarán a generar los diferentes modelos de regresión realizados en este punto:

```{r  message=FALSE, warning=FALSE}
obtenFormulaGAM<-function(dataset,varPrincipal,modelo_continuas,variableNames,variablesEliminar=variablesEliminarCorr)
{
  variablesFinal <- c()
  factores <- names(sapply(dataset,is.factor)[sapply(dataset,is.factor) == TRUE])
  
  
        for(variable in modelo_continuas) 
        {
          #nuevoValor <- paste('s(',variable,',bs="ps",m=2,k=10,by = Neighborhood)')
          #nuevoValor2 <- paste('s(',variable,',bs="ps",m=2,k=10')
          if(grepl('^[0-9]', variable)) {
            numeroElementosDistintos = 99
          } else {
            numeroElementosDistintos = dim(unique(dataset[variable]))[1]
          }
          numeroK <- numeroElementosDistintos
          if(numeroK > 9){
            numeroK <- -1
          }
          
          if(!variable  %in%  factores) {
            #nuevoValor <- paste('s(',variable,',by = MSSubClass)')
            if(grepl('^[0-9]', variable)) {
                nuevoValor2 <- paste('s(X',variable,', k= ',numeroK,')',collapse='',sep = "")
            } else {
               nuevoValor2 <- paste('s(',variable,', k= ',numeroK,')')
            }
          } else {
            if(grepl('^[0-9]', variable)) {
              nuevoValor2 <- paste('X',variable,collapse='',sep = "")
            } else {
              nuevoValor2 <- paste(variable)
            }
            
          }
          #variablesFinal <- c(variablesFinal,nuevoValor2,nuevoValor)
          
          if(numeroElementosDistintos > 1 && ! variable %in% variablesEliminar ) {
            variablesFinal <- c(variablesFinal,nuevoValor2)
          }
  }
  #print(variablesFinal)
  as.formula(paste(varPrincipal, paste(variablesFinal, collapse=" + "), sep=" ~ "))
}


obtenFormulaGAMCaret <-function(dataset,varPrincipal,modelo_continuas,variableNames,variablesEliminar=variablesEliminarCorr)
{
  variablesFinal <- c()
  factores <- names(sapply(dataset,is.factor)[sapply(dataset,is.factor) == TRUE])
  
  
        for(variable in modelo_continuas) 
        {
          
          if(grepl('^[0-9]', variable)) {
              numeroElementosDistintos = 999
          } else {
              numeroElementosDistintos = dim(unique(dataset[variable]))[1]
          }
          numeroK <- numeroElementosDistintos
          if(numeroK > 9){
            numeroK <- -1
          }
          
          if(!variable  %in%  factores) {
            #nuevoValor <- paste('s(',variable,',by = MSSubClass)')
            if(grepl('^[0-9]', variable)) {
                nuevoValor2 <- paste('X',variable,collapse='',sep = "")
            } else {
                nuevoValor2 <- paste(variable)
            }
          } else {
             if(grepl('^[0-9]', variable)) {
                  nuevoValor2 <- paste('X',variable,collapse='',sep = "")
             } else {
                  nuevoValor2 <- paste(variable)
             }
           
          }
         
          
          if(numeroElementosDistintos > 1 && ! variable %in% variablesEliminar ) {
            variablesFinal <- c(variablesFinal,nuevoValor2)
          }
  }
 
  as.formula(paste(varPrincipal, paste(variablesFinal, collapse=" + "), sep=" ~ "))
}

```

 A continuación generaremos los tres modelos de regresión comentados con anterioridad: un primer modelo con las 30 variables más importantes para el modelo que intente estimar el valor de una vivienda. Otro modelo que se concentre en las 20 características más importantes que definen las casas "baratas" y otro con las 13 características más importantes que definen las casas "caras". El siguiente código genera el primero de los modelos:

```{r  message=FALSE, warning=FALSE}

formulaCarosBaratos <- obtenFormulaGAM(dataSetTrainGAM,c('SalePrice'),dataframeTestImportantes[1:30,]$Variables)
formulaBaratos <- obtenFormulaGAM(dataSetTrainGAMBaratos,c('SalePrice'),dataframeTestImportantesBaratos[1:20,]$Variables)
formulaCaros <- obtenFormulaGAM(dataSetTrainGAMCaros,c('SalePrice'),dataframeTestImportantesCaros[1:20,]$Variables)



modeloGAMCarosBaratos <- mgcv::gam( formulaCarosBaratos  , data = dataSetTrainGAM , family=gaussian, method="GCV.Cp",  cluster=cl ,control=list(nthreads=4),select = TRUE)


#summary(modeloGAMCarosBaratos)
#plot(modeloGAMCarosBaratos,all.terms=TRUE,residuals=TRUE)
#mgcv::gam.check(modeloGAMCarosBaratos)
#concurvity(modeloGAMCarosBaratos,full=TRUE)

```

  Este es el código que genera el modelo de las casas "baratas":

```{r message=FALSE, warning=FALSE}

#b1 <- mgcv::gam(formula, data = datasetTrainGam2 , family=gaussian )
modeloGAMBaratos <- mgcv::gam( formulaBaratos  , data = dataSetTrainGAM , family=gaussian, method="GCV.Cp",  cluster=cl ,control=list(nthreads=4))


#summary(modeloGAMBaratos)
#plot(modeloGAMBaratos,all.terms=TRUE,residuals=TRUE)
#mgcv::gam.check(modeloGAMBaratos)

```

  Y este es el código que genera el modelo de las casas "caras":

```{r message=FALSE, warning=FALSE}

modeloGAMCaros <- mgcv::gam( formulaCaros  , data = dataSetTrainGAM , family=gaussian, method="GCV.Cp",  cluster=cl ,control=list(nthreads=4))




```

  A continuación veremos las características y poder de predicción de estos tres modelos generados con los elementos de Train del dataset completo sobre los elementos de Test. para ello, calcularemos las correlaciones de los elementos predecidos y la métrica MAE. El código que realiza este proceso se muestra a continuación:

```{r message=FALSE, warning=FALSE, error=FALSE}

predictedCarosBaratosArray = matrix(, ncol = 3,nrow=nrow(dataSetTestGAM))
for (index in seq(1,nrow(dataSetTestGAM))) {
  try(
    { 
      predictedCarosBaratosArray[index,1] = index
      predictedCarosBaratosArray[index,3] = dataSetTestGAM[index,]$SalePrice
      predictedCarosBaratos <- predict(modeloGAMCarosBaratos,dataSetTestGAM[index,] , type="response")
      predictedCarosBaratosArray[index,2] = predictedCarosBaratos[1]
    }
    )
}


predictedCarosArray = matrix(, ncol = 3,nrow=nrow(dataSetTestGAM))
for (index in seq(1,nrow(dataSetTestGAM))) {
  try(
    { 
      predictedCarosArray[index,1] = index
      predictedCarosArray[index,3] = dataSetTestGAM[index,]$SalePrice
      predictedCaros <- predict(modeloGAMCaros,dataSetTestGAM[index,] , type="response")
      predictedCarosArray[index,2] = predictedCaros[1]
    }
    )
}

predictedBaratosArray = matrix(, ncol = 3,nrow=nrow(dataSetTestGAM))
for (index in seq(1,nrow(dataSetTestGAM))) {
  try(
    { 
      predictedBaratosArray[index,1] = index
      predictedBaratosArray[index,3] = dataSetTestGAM[index,]$SalePrice
      predictedBaratos <- predict(modeloGAMBaratos,dataSetTestGAM[index,] , type="response")
      predictedBaratosArray[index,2] = predictedBaratos[1]
    }
    )
}

mae <- function(error) { mean(abs(error), na.rm=TRUE) }


corCarosBaratos <- cor(predictedCarosBaratosArray,use="complete.obs")[2,3]
corCaros <- cor(predictedCarosArray,use="complete.obs")[2,3]
corBaratos<- cor(predictedBaratosArray,use="complete.obs")[2,3]

maeCarosBaratos <- mae(predictedCarosBaratosArray[,2]-predictedCarosBaratosArray[,3])
maeCaros <- mae(predictedCarosArray[,2]-predictedCarosArray[,3])
maeBaratos <- mae(predictedBaratosArray[,2]-predictedBaratosArray[,3])

```

  La siguiente función muestra los valores AIC para los 3 modelos. Como se ve, el modelo denominado "modeloGAMCarosBaratos" es ligeramente mejor que el resto de los modelos. Con lo que es un punto más para seleccionar dicho modelo frente a los otros dos:

```{r message=FALSE, warning=FALSE}
AIC(modeloGAMCarosBaratos,modeloGAMBaratos,modeloGAMCaros)

```


  Con estos resultados en la mano, elegiremos como mejor modelo para predecir el valor de las casas (el logaritmo del mismo) a el modelo que no discrimina entre las características de las casas baratas y las caras (parece un poco de sentido común que al final el mejor modelo sea esta).
  
  Comprobaremos la validad del modelo en primer lugar mostrando las típicas gráficas que muestran la distribución de los resíduos para comprobar si las hipótesis básicas para las cuales un modelo lineal es válido se cumplen (homocedasticidad de los residuos, incorrelación de los residuos, y normalidad de los residuos).  Comprobamos que la hipótesis de homocedasticida no se cumple en el modelo, existe todavía ciertos cambios en la varianza de los errores en función de la predicción (una ligera parábola hacia arriba). Hay que seguir trabajando en el ajuste de los parámetros de este modelo para conseguir cumplir con este criterio.


```{r message=FALSE, warning=FALSE}

mgcv::gam.check(modeloGAMCarosBaratos)

```

  Ahora conprobaremos en el modelo si hay correlación elevada entre las variables. El concepto equivalente en los modelos de tipo GAM se denomina "concurvity", y la siguiente función ayuda a mostrar este rasgo en los modelos GAM:

```{r message=FALSE, warning=FALSE}

concurvity(modeloGAMCarosBaratos)

```

   Se comprueba que, efectivamente, existe un elevado grado de correlación entre alguna de las combinaciones de las variables "GrLivArea", "LotArea", "TotRmsAbvGrd" y "TotalBsmtSF". Está claro que sí que se puede dar esa correlación entre "GrLivArea" , "TotalBsmsSF" y "TotRmsAbvGrd", debido a que todas tienen relacióncon el área de la planta del edificio, pero no encuentro razón para que la variable "LotArea" aparezca en este listado.
   
   A continuación cargaremos el modelo de regresión logística obtenido en la parte de modelos de clasificación:


```{r message=FALSE, warning=FALSE,  error=FALSE}

invisible({capture.output({

 modeloGAMLogisticRegressionCaros <- readRDS("modeloGAMLogisticRegressionCarosCaret.rds")
 
})})

```

  Y por último generamos el modelo final con el siguiente algoritmo: en primer lugar dado un elemento del dataset y utilizando el modelo de clasificacion en baratos y caros de GAMLogit, decidimos si es un piso barato o caro. Calculamos dos modelos de regresión lineal uno con las población de baratos y otro con la población de caros únicamente.  Con la decisión que nos ha dado el algoritmo de clasificación devolvemos una media ponderada de los precios en la que los pesos de dicha media son las probabilidades de que una casa sea barata o cara. Como vemos por los resultados, esta forma de juntar los dos modelos separados no parece funcionar correctamente y desde luego no funciona mejor que el modelo de regresión realizado sobre toda la población de TRAIN.


```{r message=FALSE, warning=FALSE,  error=FALSE}

invisible({capture.output({

prediccionModeloFinal <- function(dato,clasificador = modeloGAMLogisticRegressionCaros,modeloCaros = modeloGAMCaros ,modeloBaratos = modeloGAMBaratos,umbralDecision = 0.5,modoDecision = 0) {
  
  prediccionModeloClasificacion = 0.05
  returnValue = 0
  
  try(
    { 
       prediccionModeloClasificacion <- predict(modeloGAMLogisticRegressionCaros,dato , type="prob")[2]
       prediccionModeloCaros <-  predict(modeloCaros,dato)
       prediccionModeloBaratos <-  predict(modeloBaratos,dato)
       if(modoDecision == 1) {
          returnValue <- (1-prediccionModeloClasificacion)*prediccionModeloCaros + (prediccionModeloClasificacion)*prediccionModeloBaratos
       } else {
         if (prediccionModeloClasificacion < umbralDecision ) {
            returnValue <- prediccionModeloBaratos
         } else {
            returnValue <- prediccionModeloCaros
         }
       }
     
      
    }
    )
  
  if(returnValue == 0) {
     try(
     { 
       
       prediccionModeloCaros <-  predict(modeloCaros,dato)
       prediccionModeloBaratos <-  predict(modeloBaratos,dato)
       if(modoDecision == 1) {
       returnValue <- (1-prediccionModeloClasificacion)*prediccionModeloCaros + (prediccionModeloClasificacion)*prediccionModeloBaratos
       } else {
         returnValue <- prediccionModeloBaratos
       }
       
     })
  }
  
  if(returnValue == 0) {
    returnValue = NA
    
  }
  
  return(returnValue)
  
}


predictedModeloFinalArray = matrix(, ncol = 3,nrow=nrow(dataSetTestGAM))
for (index in seq(1,nrow(dataSetTestGAM))) {
  try(
    { 
      predictedModeloFinalArray[index,1] = index
      predictedModeloFinalArray[index,3] = dataSetTestGAM[index,]$SalePrice
      predictedModeloFinal <- prediccionModeloFinal(dataSetTestGAM[index,])
      predictedModeloFinalArray[index,2] = unlist(predictedModeloFinal[1])
    }
    )
}


})})

plot(predictedModeloFinalArray[,3], predictedModeloFinalArray[,3] - predictedModeloFinalArray[,2], ylab="Residuals", xlab="Waiting Time",  main="Old Faithful Eruptions") 
abline(0, 0)    


corModeloTotal<- cor(predictedModeloFinalArray,use="complete.obs")[2,3]
maeModeloTotal <- mae(predictedModeloFinalArray[,2]-predictedBaratosArray[,3])


```




