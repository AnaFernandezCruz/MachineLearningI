---
title: "DECISION TREES"
output: 
  html_document:
    code_folding: hide
---

En este punto vamos a aplicar los árboles de decisión a nuestro modelo. Los árboles de decisión se componen del nodo raíz, que es del que partes, y respondiendo preguntas con "si" o "no", nos iremos moviendo por los nodos interiores hasta llegar a las hojas o nodos terminales. Nuestro objetivo es tener un árbol de decisión que use el menor número de preguntas posible.

Para empezar, cargamos nuestros datos.

```{r message = FALSE}
library(rpart)
library(rpart.plot)
library(rattle)
library(tidyverse)
library(readr)

var_modelo = c( 'GrLivArea','LotArea','Total_Bathrooms','Total_porch_SF','LandContour','LotConfig','Condition1','BldgType','HouseStyle','MasVnrType','Foundation','SaleType','SaleCondition','OverallQual','OverallCond','PavedDrive','Fence','BsmtFinSF1','ExterQual','BsmtQual','BsmtExposure','BsmtFinType1','BsmtUnfSF','KitchenQual','Fireplaces','FireplaceQu','BedroomAbvGr','LotShape','GarageArea','GarageCond','Neighborhood','has2ndFloor','All#ey','TotalSF','GarageType','MasVnrArea','MSSubClass','GrupoPrecio')

dataTrain <- readRDS("datasetTrain.rds") 
myvars <- names(dataTrain) %in% c(var_modelo)
dataTrain <- dataTrain[myvars]

dataTest <- readRDS("datasetTest.rds")
myvars <- names(dataTest) %in% c(var_modelo)
dataTest <- dataTest[myvars]
                       
val <- readRDS("datasetValidation.rds")
myvars <- names(val) %in% c(var_modelo)
val <- val[myvars]
set.seed(123)
```

Definimos el árbol de decisión: para ello, como no conocemos ninguna relación con el resto de variables, no escribiremos ninguna fórmula. El árbol que obtenemos es el siguiente:
```{r fig.width=30, fig.height=10}
HouseTREE.1 = rpart(GrupoPrecio ~ ., data = dataTrain)

par(mfrow = c(1, 1), xpd = NA) # otherwise on some devices the text is clipped
fancyRpartPlot(HouseTREE.1, sub = "")

```
Podemos observar como los tipos de casas se han ido dividiendo según las decisiones tomadas en cada uno de los nodos del árbol. También podemos ver como en los nodos terminales aparece la distribución de la variable GrupoPrecios, así que podemos hacer una estimación del error de clasficación del árbol con las herramientas vistas en clase. Para ello:

```{r}
##CÁLCULO DEL ERROR A LA HORA DE CLASIFICAR:
table(pred = predict(HouseTREE.1, type = "class"), obs = dataTrain$GrupoPrecio)
n = dim(dataTrain)[1]
error1 = 100 * sum(predict(HouseTREE.1, type = "class") != dataTrain$GrupoPrecio)/n
error1

##RESULTADO EN EL DATASET DE PRUEBA:
tab1 = table(pred = predict(HouseTREE.1, dataTest, type = "class"), obs = dataTest$GrupoPrecio)
ntest1 = nrow(dataTest)
acierto1 = sum(diag(tab1))/ntest1
tab1
acierto1
```

Un error de poco más del 9% mientras que obtenemos un accuracy del 83,77% para el dataset de test. Con estos valores podemos entender que nuestro modelo puede estar sobreentrenado y tendremos que realizar una poda del mismo. Antes de ponernos manos a la masa para realizar la poda del árbol, vamos a proceder a alterar los hiperparámetros del árbol de decisión. En este caso vamos a meter las probabilidades a priori en el árbol y ver cómo varía el resultado. Para ello, obtenemos el conteo de cuantos elementos pertenecen a cada clase y junto al total de las observaciones podemos sacar la probabilidad a priori.

```{r}
library(ggplot2)
ggplot(data.frame(dataTrain), aes(x=dataTrain$GrupoPrecio)) + geom_bar()

as.data.frame(table(dataTrain$GrupoPrecio))

length(dataTrain$GrupoPrecio)
```
Teniendo esta información, metemos la probabilidad a priori.
```{r fig.width=30, fig.height=10}
##AJUSTE HIPERPARÁMETROS:
prob1 = 1581 / 1837
prob2 = 256 / 1837

HouseTREE.2 = rpart(GrupoPrecio ~ ., data = dataTrain, parms = list(prior = c(prob1,prob2), split = "information"))
par(mfrow = c(1, 1), xpd = NA) # otherwise on some devices the text is clipped
fancyRpartPlot(HouseTREE.2, sub = "")
```
Los resultados obtenidos son los siguientes:
```{r}
##CÁLCULO DEL ERROR A LA HORA DE CLASIFICAR:
table(pred = predict(HouseTREE.2, type = "class"), obs = dataTrain$GrupoPrecio)
n = dim(dataTrain)[1]
error2 = 100 * sum(predict(HouseTREE.2, type = "class") != dataTrain$GrupoPrecio)/n
error2

##RESULTADO EN EL DATASET DE PRUEBA:
tab2 = table(pred = predict(HouseTREE.2, dataTest, type = "class"), obs = dataTest$GrupoPrecio)
ntest2 = nrow(dataTest)
acierto2 = sum(diag(tab1))/ntest2
tab2
acierto2
```

Vemos que después de añadir las probabilidades a priori el accuracy del modelo se mantiene constante aunque el error ha disminuido sutilmente en este segundo modelo. Vamos a proceder a seguir nuestro estudio con el segundo árbol que hemos entrenado ya que, a accuracys similares, el segundo tiene menos error. 

Para podar el modelo usaremos las herramientas printcp y plotcp. Con estas herramientas podemos tener el número óptimo de podas. La poda la haremos para evitar el overfitting, como en nuestro caso, ya que con un accuracy de un 0.84 podemos asumir que tenemos un problema de overfitting entre el Train y el Test. Lo que buscaremos será quedarnos con el árbol más pequeño con el menor error obtenido haciendo cross validation. Utilizaremos el primer árbol, pues parece que da menos error y puede tener menos overfitting.

```{r}
##PODA:
printcp(HouseTREE.2)
plotcp(HouseTREE.2)
```

Plotcp nos devulve una representación gráfica con un resumen del error obtenido haciendo cross validation. Los valores de CP (Complexity Parameter) se dibujan en el eje x mientras que en la y tenemos el valor de la media geométrica para representar la desviación hasta  alcanzar el valor mínimo. Vamos que el error relativo es menor para un valor de cp de 0,012. Lo comprobamos:

```{r fig.width=30, fig.height=10}
pruneTREE.1 = prune(HouseTREE.1,cp = HouseTREE.1$cptable[which.min(HouseTREE.1$cptable["xerror"]), "CP"])
fancyRpartPlot(pruneTREE.1, uniform = TRUE, main = "Pruned Classification Tree", sub = "")

```
Si probamos el árbol podado:

```{r}
##CÁLCULO DEL ERROR A LA HORA DE CLASIFICAR:
table(pred = predict(pruneTREE.1, type = "class"), obs = dataTrain$GrupoPrecio)
n = dim(dataTrain)[1]
error3 = 100 * sum(predict(pruneTREE.1, type = "class") != dataTrain$GrupoPrecio)/n
error3

##RESULTADO EN EL DATASET DE PRUEBA:
tab3 = table(pred = predict(pruneTREE.1, dataTest, type = "class"), obs = dataTest$GrupoPrecio)
ntest3 = nrow(dataTest)
acierto3 = sum(diag(tab2))/ntest3
tab3
acierto3
```
Comprobamos que, efectivamente, al coger el valor mínimo de cp el árbol se ha podado y el valor de la accuracy ha disminuido, disminuyendo el overfitting.

- COMPARACIÓN DE MODELOS.

Para realizar la comparación de los dos (o tres) árboles que hemos creado, acudimos a las estadísticas que hemos obtenido.

Las estadísticas las sacamos sobre el conjunto de test. Para el primer árbol:
```{r}
tab1
acierto1
```

Las estadísticas para el segundo árbol donde hemos añadido el hiperparámetro con las probabilidades a priori:
```{r}
tab2
acierto2
```

Las estadísticas con el árbol podado:
```{r}
tab3
acierto3
```

Como vemos, cuando hemos añadido las probabilidades a priori hemos mejorado la predicción en las casas baratas. Vemos que en nuestro caso es un modelo totalmente desbalanceado ya que tenemos una clara predominación de la clase "Barato" y somos muy buenos prediciendo cuando una casa es barata. Es por ello por lo que los resultados son bastante similares, ya que aunque añadamos las probabilidades a priori solo reforzamos ese aspecto del entrenamiento. Como hemos dicho además en la poda, esto solo ha hecho que el modelo se quede como está pues así ya estamos en el punto con el menor error relativo aunque sea un árbol complejo. 

- CURVA ROC.

```{r}
library(rpart)
library(caret)
```

Nos vamos a quedar con el modelo podado para hacer el estudio de la curva ROC. 

```{r}
pred = predict(pruneTREE.1, dataTest, type = "class")
table = table(pred, obs = dataTest$GrupoPrecio, dnn = c("Actual", "Predichos"))

confusionMatrix(table)


draw_confusion_matrix <- function(tab, tab_Actual, tab_Predict){
  confusion_matrix <- as.data.frame(tab)
  
  Actual <- factor(confusion_matrix[[tab_Actual]])
  Predichos <- factor(confusion_matrix[[tab_Predict]])
  Y      <- confusion_matrix$Freq
  df <- data.frame(Actual, Predichos, Y)
  
  ggplot(data =  df, mapping = aes(x = Actual, y = Predichos)) +
    geom_tile(aes(fill = Y), colour = "white") +
    geom_text(aes(label = sprintf("%1.0f", Y)), vjust = 1) +
    scale_fill_gradient(low = "blue", high = "red") +
    theme_bw() + theme(legend.position = "none")
}


draw_confusion_matrix(table, "Actual", "Predichos")


library(pROC)
predRoc <- predict(pruneTREE.1, newdata=dataTest, type="prob")
area_curva <- multiclass.roc(dataTest$GrupoPrecio, predRoc,)
area_curva

        
```